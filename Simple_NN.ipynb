{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple_NN\n",
    "Class to create a simple neural network\n",
    "    Learning Algorithm: stochastic gradient descent with backpropagation\n",
    "    Activation Function: sigmoid\n",
    "    Cost Function: MSE\n",
    "\"\"\"\n",
    "class Simple_NN(object):\n",
    "    \"\"\" \n",
    "    INITIALIZE THE NETWORK\n",
    "    \"\"\"\n",
    "    def __init__(self, layers, activation_function=\"sigmoid\", cost_function=\"MSE\"):\n",
    "        \"\"\"\n",
    "        self.layers is a list of numbers where the ith number how many neurons are in\n",
    "        the ith layer of the network.\n",
    "        \"\"\"\n",
    "        self.layers = layers;\n",
    "        self.num_layers = len(layers);\n",
    "        \n",
    "        \"\"\"\n",
    "        self.weights[Layer - 1, input_neuron, output_neuron] = \n",
    "                                            List of weight matrices for each layer.                      \n",
    "        self.biases[Layer - 1, neuron] = \n",
    "                                            List of vectors with biases for each neuron  \n",
    "        FOR EXAMPLE:\n",
    "            self.weights[layer, j, i] = weight going into the jth neuron of the lth layer\n",
    "                                    from the ith neuron of the (l-1)st layer \n",
    "            self.biases[layer, k] = bias on the kth nuron of the lth layer\n",
    "        NOTE: layer 0 is the input layer, so self.weights[0] is the weights going into layer 1\n",
    "        \"\"\"\n",
    "        self.weights = [];\n",
    "        self.biases = [];\n",
    "        self.Z = [];\n",
    "        self.activations = [];\n",
    "        # Create matrices with correct dimensions \n",
    "        for layer_num in range(1, self.num_layers):\n",
    "            self.weights.append(np.random.randn(layers[layer_num], layers[layer_num - 1]));\n",
    "            self.biases.append(np.random.randn(layers[layer_num]));\n",
    "        \"\"\"\n",
    "        self.activation = string specifying what activation function the neurons will use. \n",
    "            The options are:\n",
    "            sigmoid (default)\n",
    "        self.cost_function = string specifying hat cost function will be used for the \n",
    "            network.\n",
    "            The options are:\n",
    "            MSE (default)\n",
    "        \"\"\"\n",
    "        self.activation_function = activation_function;\n",
    "        self.cost_function = cost_function;\n",
    "        self.errors = [];\n",
    "        self.gradC_b = [];\n",
    "        self.gradC_w = [];\n",
    " \n",
    "    \"\"\"\n",
    "    TRAINING\n",
    "    Train the network using stochastic gradient descent and backpropagation.\n",
    "    Training data should be given in the following format:\n",
    "        [x11, x12, ..., x1i, y1\n",
    "         x21, x22, ..., x2i, y2\n",
    "         ...\n",
    "         xm1, xm1, ..., xmi, ym]\n",
    "    Where each row corrsponds to a training example with i data points\n",
    "    \"\"\"\n",
    "    def train(self, training_data, batch_size, num_epochs, learning_rate):\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"EPOCH: %d\" % (epoch + 1));\n",
    "            # Randomize the order of training examples\n",
    "            np.random.shuffle(training_data);\n",
    "            # Separate inputs from outputs\n",
    "            inputs = np.matrix(training_data[:, :-1]);\n",
    "            outputs = np.matrix(training_data[:, -1]);\n",
    "            # For each epoch, loop through each batch to use as training data\n",
    "            for batch in range(len(training_data))[::batch_size]:\n",
    "                \"\"\"\n",
    "                For each batch, we calculate activations and use the backpropagation\n",
    "                algorithm to change the weights and biases using gradient descent\n",
    "                Create matrix out of all training inputs in the batch\n",
    "                To apply W to all input vectors, we can multiply WX where\n",
    "                X is the ixm matrix containing all m training examples as columns\n",
    "                \"\"\"\n",
    "                X = inputs[batch : batch + batch_size];\n",
    "                X = np.transpose(X);\n",
    "                Y = outputs[batch : batch + batch_size];\n",
    "                # FEEDFORWARD\n",
    "                self.evaluate(X);\n",
    "                # BACKPROPAGATION\n",
    "                self.backpropagate(X, Y);\n",
    "                # GRADIENT DESCENT\n",
    "                self.grad_descent();\n",
    "    \n",
    "    \"\"\"\n",
    "    EVALUATE\n",
    "    Takes an input column vector X and computes the output of the network\n",
    "    \"\"\"\n",
    "    def evaluate(self, X):\n",
    "        \"\"\"\n",
    "        self.Z[layer, neuron, training_example] = \n",
    "                        List of vectors with weighted inputs to the neurons\n",
    "        self.activations[layer, neuron, training_Example] = \n",
    "                        List of vectors with activations for each neuron\n",
    "        \"\"\"\n",
    "        self.Z = [];\n",
    "        self.activations = [];\n",
    "        # Calclate outputs going forwards through the network\n",
    "        for layer in range(self.num_layers - 1):\n",
    "            if layer == 0:\n",
    "                # Feed inputs to the network\n",
    "                prev_activations = X;\n",
    "            else:\n",
    "                prev_activations = self.activations[layer - 1];\n",
    "            # Bias matrix where each column is a copy of the bias vector is needed\n",
    "            # to add bias terms for each training example. \n",
    "            one_vector = np.ones(np.matrix(X).shape[1]);\n",
    "            bias_matrix = np.outer(self.biases[layer], one_vector);\n",
    "            self.Z.append(np.dot(self.weights[layer], prev_activations) + bias_matrix);\n",
    "            self.activations.append(self.activation(self.Z[layer]));\n",
    "        \n",
    "    \"\"\"\n",
    "    BACKPROPAGATE\n",
    "    Using input vector X and output vector Y, calculates the following instance variables:\n",
    "        self.errors[layer, neuron, training_example] - error in each neuron\n",
    "        self.gradC_b[layer] - average gradient wrt biases over all training examples\n",
    "        self.gradC_w[layer, input_neuron, output_neuron] - average gradient wrt weights\n",
    "    \"\"\"\n",
    "    def backpropagate(self, X, Y):\n",
    "        # Calculate output error matrix so the [i, j]th entry contains the\n",
    "        # error for the ith neuron in the output layer for the jth training\n",
    "        # example\n",
    "        output_error = np.multiply(\n",
    "                             self.cost_derivative(self.activations[-1], Y),\n",
    "                             self.activation_derivative(self.Z[-1]));\n",
    "        # Backpropogate: we create the errors matrix which is indexed\n",
    "        # in the form errors[layer, neuron, training_example]\n",
    "        self.errors = [output_error];\n",
    "        # Note that in the loop, we use negative subscripts to go through the \n",
    "        # layers from output towards input. We therefore start at layer [-2], the\n",
    "        # second to last layer\n",
    "        for layer in range(2, self.num_layers):               \n",
    "            # For each layer, calculate errors in previous layer\n",
    "            previous_errors = np.multiply(\n",
    "                                np.dot(\n",
    "                                    np.transpose(self.weights[-layer + 1]),\n",
    "                                    self.errors[0]), \n",
    "                                self.activation_derivative(self.Z[-layer])); \n",
    "            # Add previous errors to the beginning of the error matrix list\n",
    "            self.errors.insert(0, previous_errors);\n",
    "        # Calculate gradients of cost function\n",
    "        self.gradC_b = [];\n",
    "        self.gradC_w = [];\n",
    "        for layer in range(self.num_layers - 1):\n",
    "            \"\"\"\n",
    "            gradC_b[layer, neuron] = \n",
    "            Gradient of cost wrt biases for a layer is just the\n",
    "            vector of errors for that layer.\n",
    "            When we compute the average over all training examples using \n",
    "            np.average(matrix, 1), we get a column vector. In order to\n",
    "            correctly subtract this from the row-vector of biases, \n",
    "            we transpose the gradient vectors.\n",
    "            \"\"\"\n",
    "            self.gradC_b.append(np.average(self.errors[layer],1));\n",
    "            \"\"\"\n",
    "            sum_of_weights[layer, j, k] will contain the partial derivative \n",
    "            of cost wrt the weight from the kth neuron in layer - 1 to the jth\n",
    "            neuron in layer summed over all training examples. That is,\n",
    "            sum_of_weightes[layer, j, k] = [sum over training examples dC/dw_j,k]\n",
    "            \"\"\"\n",
    "            if (layer == 0):\n",
    "               prev_activations = X;\n",
    "            else:\n",
    "               prev_activations = self.activations[layer - 1];\n",
    "            sum_of_weights = np.dot(\n",
    "                                    self.errors[layer],\n",
    "                                    np.transpose(prev_activations));\n",
    "            \"\"\"\n",
    "            gradC_w [layer, input_neuron, output_neuron] is also \n",
    "                averaged over all traininge examples in the batch\n",
    "            \"\"\"\n",
    "            self.gradC_w.append((1 / batch_size) * sum_of_weights);\n",
    "    \n",
    "    \"\"\"\n",
    "    GRADIENT DESCENT\n",
    "    Use the gradients computed in backpropagate() to update the weights and\n",
    "    biases of the network.\n",
    "    \"\"\"\n",
    "    def grad_descent(self):\n",
    "        for layer in range(self.num_layers - 1):\n",
    "            self.biases[layer] = np.subtract(self.biases[layer],\n",
    "                        np.transpose(np.multiply(learning_rate, self.gradC_b[layer])));\n",
    "            self.weights[layer] = np.subtract(self.weights[layer],\n",
    "                        np.multiply(learning_rate, self.gradC_w[layer]));\n",
    "        \n",
    "    \"\"\" \n",
    "    ACTIVATION FUNCTION\n",
    "    For this network, we use the sigmoid function to calculate neuron activation\n",
    "    In general, we assume the input z will be a matrix where the [i,j]th entry is the\n",
    "    ith neuron in the jth training example\n",
    "    \"\"\"      \n",
    "    def activation(self, z):\n",
    "        if (self.activation_function == \"sigmoid\"):\n",
    "            return 1.0 / (1 + np.exp(-z));\n",
    "    \n",
    "    def activation_derivative(self, z):\n",
    "        if (self.activation_function == \"sigmoid\"):\n",
    "            return np.multiply((1 - self.activation(z)), self.activation(z));\n",
    "    \"\"\"\n",
    "    COST FUNCTION\n",
    "    We assume that activations is a 2D matrix where each column corresponds to the activations\n",
    "    for a specific training example and each row corresponds to a specific neuron\n",
    "\n",
    "    We assume for now that outputs for the network are disjoint categories, so only\n",
    "    one output neuron should fire at a time. The output_matrix function turns the output\n",
    "    vector where each entry corresponds to a training example into a matrix where each\n",
    "    column corresponds to a training example, with only one nonzero entry per column\n",
    "    corresponding to the output for that example\n",
    "    \n",
    "    Both calculate_cost and cost_derivative return a row vector where the ith entry\n",
    "    is the cost/cost derivative for training example i\n",
    "    \"\"\"\n",
    "    def output_matrix(self, activations, outputs):\n",
    "        # output_matrix[neuron, training_example]\n",
    "        output_matrix = np.zeros((activations.shape[0], outputs.shape[0]));\n",
    "        for training_example, output in enumerate(outputs):\n",
    "            output_matrix[output - 1, training_example] = 1;\n",
    "        return output_matrix;\n",
    "    \n",
    "    def calculate_cost(self, activations, outputs):\n",
    "        outputs_matrix = self.output_matrix(activations, outputs);\n",
    "        if (self.cost_function == \"MSE\"):\n",
    "            squared_errors = np.square(activations - output_matrix);\n",
    "            # Average sum of squared errors over each training example\n",
    "            return 0.5 * np.average(np.sum(squared_errors, 0));\n",
    "    \"\"\"\n",
    "    Derivative of cost functions with respect to activations\n",
    "    Each entry [i, j] in the resulting matrix will be the gradient of the cost function\n",
    "    with respect to the activation of the ith neuron in the jth training example\n",
    "    \"\"\"\n",
    "    def cost_derivative(self, activations, outputs):\n",
    "        output_matrix = self.output_matrix(activations, outputs);\n",
    "        if (self.cost_function == \"MSE\"):\n",
    "            return (activations - output_matrix);\n",
    "       \n",
    "    \"\"\" \n",
    "    TODO: \n",
    "        predict(data) -> call evaluate, return neuron number in output layer with max output\n",
    "        \n",
    "        test(testing_data) -> Add to each epoch in train to get # of correct predictions\n",
    "            using predict and Y\n",
    "    \"\"\"\n",
    "    \n",
    "    def print_network(self, debug=0):\n",
    "        print(\"Number of layers: \");\n",
    "        print(self.num_layers);\n",
    "        print(\"Weights: \")\n",
    "        for layer in self.weights:\n",
    "            print(layer)\n",
    "        print(\"\\nBiases:\" )\n",
    "        for layer in self.biases:\n",
    "            print(layer)\n",
    "        print(\"\\nActivations:\")\n",
    "        for layer in self.activations:\n",
    "            print(layer);\n",
    "        # Print extra info about the network\n",
    "        if (debug == 1):\n",
    "            print(\"\\nWeighted Inputs:\")\n",
    "            for layer in self.Z:\n",
    "                print(layer)\n",
    "            print(\"\\nErrors:\")\n",
    "            for layer in self.errors:\n",
    "                print(layer)\n",
    "            print(\"\\nBias Gradients:\")\n",
    "            for layer in self.gradC_b:\n",
    "                print(layer)\n",
    "            print(\"\\nWeight Gradients\")\n",
    "            for layer in self.gradC_w:\n",
    "                print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\n",
      "EPOCH: 2\n",
      "EPOCH: 3\n",
      "EPOCH: 4\n",
      "EPOCH: 5\n",
      "EPOCH: 6\n",
      "EPOCH: 7\n",
      "EPOCH: 8\n",
      "EPOCH: 9\n",
      "EPOCH: 10\n",
      "Done Training\n",
      "Number of layers: \n",
      "5\n",
      "Weights: \n",
      "[[-1.57385241 -0.98113612 -0.62163248]\n",
      " [-0.16552989 -0.08024697  0.08835607]\n",
      " [ 0.90473293 -0.78300384 -0.42353519]\n",
      " [ 0.30717572 -0.38364937 -0.36020769]\n",
      " [-0.40471432 -0.03408016 -1.05373029]\n",
      " [ 1.73568318  0.00372334  0.25753556]\n",
      " [ 0.19085972 -0.48560685  0.14483556]]\n",
      "[[ 0.55722924 -0.81221202 -0.19445222  1.57406828 -0.33503165  0.63293282\n",
      "   1.46264311]\n",
      " [-0.82334928 -2.0436962   0.98549067  2.0283247   0.13466573  2.33253479\n",
      "   0.78327198]\n",
      " [ 2.58965687  1.30094233  0.33424546  0.68120173  0.11688864  1.05984171\n",
      "   1.81946981]\n",
      " [-0.41389148 -0.14637531  1.05729996  0.08461584 -0.86509125  1.66868837\n",
      "   0.28443579]\n",
      " [ 0.15225072 -2.0888847   0.65296554  1.63008856  0.63175085 -0.87351167\n",
      "  -0.89370605]]\n",
      "[[-1.13970394 -0.79415168 -0.18912557 -0.98659181  2.19564678]\n",
      " [ 0.11983706 -1.45035694  2.19737104 -0.25051697  0.06667381]\n",
      " [ 0.77386591  1.36919142 -1.66384901 -1.38374195  0.72905955]\n",
      " [ 1.44550191 -1.17322677  0.47872132  0.56747401 -1.69253169]]\n",
      "[[-0.34835503 -0.20892251 -0.08758252 -0.21049412]\n",
      " [-0.59604345 -1.28346838 -1.43964332 -0.01649634]\n",
      " [ 0.79351073 -0.19122639 -0.09887147 -1.82617154]\n",
      " [-0.35337408 -1.07766041 -1.72065415  1.47868234]\n",
      " [ 0.21598611 -1.08477942 -0.82930167 -1.09512335]\n",
      " [-0.52418129  0.57129066 -0.94743732  0.26884583]]\n",
      "\n",
      "Biases:\n",
      "[[ 1.00169498 -0.69754509  0.86444339 -0.58731116 -0.5668202   2.45871205\n",
      "  -1.47947823]]\n",
      "[[ 1.245668    0.90810532  0.15681968 -0.79034581  0.15646901]]\n",
      "[[-1.52593184  0.01815222  2.51547602 -0.51136024]]\n",
      "[[-1.07879735  1.14055817 -0.27765529 -0.02557424 -0.61073071 -0.90878595]]\n",
      "\n",
      "Activations:\n",
      "[[ 0.0121365 ]\n",
      " [ 0.31897187]\n",
      " [ 0.25589081]\n",
      " [ 0.10639956]\n",
      " [ 0.01476115]\n",
      " [ 0.99313512]\n",
      " [ 0.13879608]]\n",
      "[[ 0.87410268]\n",
      " [ 0.95857039]\n",
      " [ 0.88775877]\n",
      " [ 0.75425567]\n",
      " [ 0.24054704]]\n",
      "[[ 0.02490942]\n",
      " [ 0.62491258]\n",
      " [ 0.89650328]\n",
      " [ 0.51834487]]\n",
      "[[ 0.19691956]\n",
      " [ 0.2737776 ]\n",
      " [ 0.19582202]\n",
      " [ 0.18483533]\n",
      " [ 0.06950189]\n",
      " [ 0.21842608]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TEST NETWORK CREATION\n",
    "\"\"\"\n",
    "test = Simple_NN([3, 7, 5, 4, 6]);\n",
    "random_data = np.matrix('1, 2, 3, 0; \\\n",
    "                        11, 12, 13, 1; \\\n",
    "                        21, 22, 23, 2; \\\n",
    "                        31, 32, 33, 3; \\\n",
    "                        41, 42, 43, 4; \\\n",
    "                        51, 52, 53, 5 \\\n",
    "                       ');\n",
    "    \n",
    "batch_size = 2;\n",
    "num_epochs = 10;\n",
    "learning_rate = 0.5;\n",
    "\n",
    "test.train(random_data, batch_size, num_epochs, learning_rate);\n",
    "print(\"Done Training\")\n",
    "\n",
    "test_input = np.matrix('1; 2; 3');\n",
    "test.evaluate(test_input);\n",
    "\n",
    "test.print_network();\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
