{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple_NN\n",
    "Class to create a simple neural network\n",
    "    Activation Function: sigmoid\n",
    "    Learning Algorithm: stochastic gradient descent with backpropagation\n",
    "    Cost Function: Mean squared error\n",
    "\"\"\"\n",
    "class Simple_NN(object):\n",
    "    \"\"\" \n",
    "    INITIALIZE THE NETWORK\n",
    "    \"\"\"\n",
    "    def __init__(self, layers, activation_function=\"sigmoid\"):\n",
    "        \"\"\"\n",
    "        self.layers is a list of numbers where the ith number how many neurons are in\n",
    "        the ith layer of the network.\n",
    "        \"\"\"\n",
    "        self.layers = layers;\n",
    "        self.num_layers = len(layers);\n",
    "        \n",
    "        \"\"\"\n",
    "        self.weights[Layer - 1, input_neuron, output_neuron] = \n",
    "                                            List of weight matrices for each layer.                      \n",
    "        self.biases[Layer - 1, neuron] = \n",
    "                                            List of vectors with biases for each neuron  \n",
    "        FOR EXAMPLE:\n",
    "            self.weights[l, j, i] = weight going into the jth neuron of the lth layer\n",
    "                                    from the ith neuron of the (l-1)st layer \n",
    "            self.biases[l, k] = bias on the kth nuron of the lth layer\n",
    "        NOTE: layer 0 is the input layer, so self.weights[0] is the weights going into layer 1\n",
    "        \"\"\"\n",
    "        self.weights = [];\n",
    "        self.biases = [];\n",
    "        \n",
    "        self.Z = [];\n",
    "        self.activations = [];\n",
    "        # Create matrices with correct dimensions \n",
    "        for layer_num in range(1, self.num_layers):\n",
    "            self.weights.append(np.random.randn(layers[layer_num], layers[layer_num - 1]));\n",
    "            self.biases.append(np.random.randn(layers[layer_num]));\n",
    "        \"\"\"\n",
    "        self.activation = string specifying what activation function the neurons will use. \n",
    "        The options are:\n",
    "            sigmoid (default)\n",
    "        \"\"\"\n",
    "        self.activation_function = activation_function;\n",
    "        \n",
    "    \"\"\" \n",
    "    ACTIVATION FUNCTION\n",
    "    For this network, we use the sigmoid function to calculate neuron activation\n",
    "    \"\"\"      \n",
    "    def activation(self, z):\n",
    "        if (self.activation_function == \"sigmoid\"):\n",
    "            return 1.0 / (1 + np.exp(-z));\n",
    "    \n",
    "    def activation_derivative(self, z):\n",
    "        if (self.activation_function == \"sigmoid\"):\n",
    "            return (1 - self.activation(z)) * self.activation(z);\n",
    "        \n",
    "    \"\"\"\n",
    "    TRAINING\n",
    "    Train the network using stochastic gradient descent and backpropagation.\n",
    "    Training data should be given in the following format:\n",
    "        [x11, x12, ..., x1i, y1\n",
    "         x21, x22, ..., x2i, y2\n",
    "         ...\n",
    "         xm1, xm1, ..., xmi, ym]\n",
    "    Where each row corrsponds to a training example with i data points\n",
    "    \"\"\"\n",
    "    def train(self, training_data, batch_size, num_epochs, learning_rate):\n",
    "        for epoch in range(1, num_epochs):\n",
    "            # Randomize the order of training examples\n",
    "            np.random.shuffle(training_data);\n",
    "            # Separate inputs from outputs\n",
    "            inputs = training_data[:, :-2];\n",
    "            outputs = training_data[:, -1];\n",
    "            # For each epoch, loop through each batch to use as training data\n",
    "            for batch in range(len(training_data))[0 :: batch_size]:\n",
    "                # Create matrix out of all training inputs in the batch\n",
    "                X = np.matrix(inputs[batch : batch + batch_size, :]); \n",
    "                Y = np.matrix(outputs[batch : batch + batch_size]);\n",
    "                # If the first layer of the network has k neurons, and each training\n",
    "                # example has i data points, then weights will be a kxi matrix \n",
    "                # so Wx_j = kx1 vector.\n",
    "                # To apply W to all input vectors, we can multiply WX where\n",
    "                # X is the ixm matrix containing all m training examples as columns\n",
    "                \n",
    "                print(X);\n",
    "                #X = np.transpose(X);\n",
    "                \n",
    "                # FEEDFORWARD\n",
    "                \"\"\"\n",
    "                self.Z[layer, training_example, neuron] = \n",
    "                                            List of vectors with weighted inputs to the neurons\n",
    "                self.activations[layer, training_example, neuron] = \n",
    "                                            List of vectors with activations for each neuron\n",
    "                \"\"\"\n",
    "                # Calclate outputs going forwards through the network\n",
    "                for layer in range(self.num_layers - 1):\n",
    "                    if layer == 0:\n",
    "                        # Feed inputs to the network\n",
    "                        prev_activations = X;\n",
    "                    else:\n",
    "                        prev_activations = self.activations[layer - 1];\n",
    "                    # Bias matrix where each column is a copy of the bias vector is needed\n",
    "                    # to add bias terms for each training example. \n",
    "                    one_vector = np.ones(len(self.biases[layer]));\n",
    "                    bias_matrix = np.outer(self.biases[layer], one_vector);\n",
    "                    self.Z.append(np.dot(self.weights[layer], prev_activations) + \\\n",
    "                                  bias_matrix);\n",
    "                    self.activations.append(self.activation(self.Z[layer]));\n",
    "            \n",
    "                # Backpropagation\n",
    "                pass\n",
    "                # Gradient Descent\n",
    "    \n",
    "    \"\"\" \n",
    "    TODO: \n",
    "        test(testing_data)\n",
    "    \"\"\"\n",
    "    def print_network(self):\n",
    "        print(\"Weights: \")\n",
    "        for layer in self.weights:\n",
    "            print(layer)\n",
    "        print(\"\\nBiases:\" )\n",
    "        for layer in self.biases:\n",
    "            print(layer)\n",
    "        print(\"\\nWeighted Inputs:\")\n",
    "        for layer in self.Z:\n",
    "            print(layer)\n",
    "        print(\"\\nActivations:\")\n",
    "        for layer in self.activations:\n",
    "            print(layer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: \n",
      "[[-2.65780809 -1.58207156 -0.45558872]\n",
      " [ 1.87957975  0.1496402  -0.22577234]\n",
      " [-0.28291263 -1.49836166 -1.19630648]\n",
      " [ 0.58258132 -0.21383262  1.20064711]\n",
      " [-0.31830545  0.08265716 -0.80310833]]\n",
      "[[ 1.69679628  1.78833209 -0.85038455  2.74153751  1.44638488]\n",
      " [-0.5515198   1.04475691  1.15470594  1.14839571  0.4113301 ]]\n",
      "\n",
      "Biases:\n",
      "[-1.99461823  0.29609005 -1.91517441  0.40174685  0.0855234 ]\n",
      "[-0.300632   -1.14226778]\n",
      "\n",
      "Weighted Inputs:\n",
      "\n",
      "Activations:\n",
      "\n",
      "\n",
      "[[1 2 3]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (5,3) and (1,3) not aligned: 3 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-156-ac9546b83b8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mrandom_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'1, 2, 3, 4, 20; 11, 12, 13, 14, 30; 21, 22, 23, 24, 40'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-154-6959069a4d75>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, training_data, batch_size, num_epochs, learning_rate)\u001b[0m\n\u001b[0;32m    106\u001b[0m                     \u001b[0mone_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m                     \u001b[0mbias_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev_activations\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m                                   \u001b[0mbias_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (5,3) and (1,3) not aligned: 3 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TEST NETWORK CREATION\n",
    "\"\"\"\n",
    "test = Simple_NN([3, 5, 2]);\n",
    "test.print_network();\n",
    "print(\"\\n\")\n",
    "\n",
    "random_data = np.matrix('1, 2, 3, 4, 20; 11, 12, 13, 14, 30; 21, 22, 23, 24, 40')\n",
    "test.train(random_data, 1, 3, 0.5);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
