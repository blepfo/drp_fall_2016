{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple_NN\n",
    "Class to create a simple neural network\n",
    "    Activation Function: sigmoid\n",
    "    Learning Algorithm: stochastic gradient descent with backpropagation\n",
    "    Cost Function: Mean squared error\n",
    "\"\"\"\n",
    "class Simple_NN(object):\n",
    "    \"\"\" \n",
    "    INITIALIZE THE NETWORK\n",
    "    \"\"\"\n",
    "    def __init__(self, layers, activation_function=\"sigmoid\"):\n",
    "        \"\"\"\n",
    "        self.layers is a list of numbers where the ith number how many neurons are in\n",
    "        the ith layer of the network.\n",
    "        \"\"\"\n",
    "        self.layers = layers;\n",
    "        self.num_layers = len(layers);\n",
    "        \n",
    "        \"\"\"\n",
    "        self.weights[Layer - 1, input_neuron, output_neuron] = \n",
    "            matrix of weights for the entire network with indices\n",
    "            FOR EXAMPLE: The weight on the edge between neuron 5 in layer 2 and \n",
    "                neuron 3 in layer 3 is self.weights[3,5,3]     \n",
    "        self.biases[Layer - 1, neuron] = matrix containing bias vectors for each layer\n",
    "            First dimension = layer number, with biases for layer 1 in self.biases[0,:]\n",
    "            Second dimension = which neuron in the layer\n",
    "            FOR EXAMPLE: The bias on the 3rd neuron in the 5th layer is in self.biases[6, 3]    \n",
    "        self.Z = weighted inputs to each neuron.\n",
    "            Z[i, j] is the weighted input to the ith neuron in the jth layer. \n",
    "        self.activations = activation from each neuron.\n",
    "            activations[i,j] is the activation output by the ith neuron in the jth layer\n",
    "        \"\"\"\n",
    "        weights = [];\n",
    "        biases = [];\n",
    "        Z = [];\n",
    "        activations = [];\n",
    "        for layer_num in range(1, self.num_layers):\n",
    "            weights.append(np.random.randn(layers[layer_num], layers[layer_num - 1]));\n",
    "            biases.append(np.random.randn(layers[layer_num]));\n",
    "            Z.append(np.zeros(layers[layer_num]));\n",
    "            activations.append(np.zeros(layers[layer_num]));\n",
    "        self.weights = np.matrix(weights);\n",
    "        self.biases = np.matrix(biases);\n",
    "        self.Z = np.matrix(Z);\n",
    "        self.activations = np.matrix(activations);\n",
    "        \"\"\"\n",
    "        self.activation = string specifying what activation function the neurons will use. \n",
    "        The options are:\n",
    "            sigmoid (default)\n",
    "        \"\"\"\n",
    "        self.activation_function = activation_function;\n",
    "        \n",
    "    \"\"\" \n",
    "    ACTIVATION FUNCTION\n",
    "    For this network, we use the sigmoid function to calculate neuron activation\n",
    "    \"\"\"      \n",
    "    def activation(self, z):\n",
    "        if (self.activation_function == \"sigmoid\"):\n",
    "            return 1.0 / (1 + np.exp(-z));\n",
    "    \n",
    "    def activation_derivative(self, z):\n",
    "        if (self.activation_function == \"sigmoid\"):\n",
    "            return (1 - self.activation(z)) * self.activation(z);\n",
    "        \n",
    "    \"\"\"\n",
    "    TRAINING\n",
    "    Train the network using stochastic gradient descent and backpropagation.\n",
    "    Training data should be given in the following format:\n",
    "        [x11, x12, ..., x1i, y1\n",
    "         x21, x22, ..., x2i, y2\n",
    "         ...\n",
    "         xm1, xm1, ..., xmi, ym]\n",
    "    Where each row corrsponds to a training example with i data points\n",
    "    \"\"\"\n",
    "    def train(training_data, batch_size, num_epochs, learning_rate):\n",
    "        for epoch in range(1, num_epochs):\n",
    "            # Randomize the order of training examples\n",
    "            np.random.shuffle(training_data);\n",
    "            # Separate inputs from outputs\n",
    "            inputs = training_data[:, :-1];\n",
    "            outputs = training_data[:, -1];\n",
    "            # For each epoch, loop through each batch to use as training data\n",
    "            for batch in range(len(training_data))[0 :: batch_size]:\n",
    "                # Create matrix out of all training inputs in the batch\n",
    "                X = np.matrix(inputs[batch : batch + batch_size, :]); \n",
    "                Y = np.matrix(outputs[batch: batch + batch_size]);\n",
    "                # We want to transpose X so that the ith column holds the data points\n",
    "                # for the ith training example\n",
    "                X = np.transpose(X);\n",
    "                # FEEDFORWARD\n",
    "                for layer in range(1, self.num_layers):\n",
    "                    pass\n",
    "                # Backpropagation\n",
    "            \n",
    "                # Gradient Descent\n",
    "    \n",
    "    \"\"\" \n",
    "    TODO: \n",
    "        test(testing_data)\n",
    "    \"\"\"\n",
    "    def print_network(self):\n",
    "        print(\"Weights: \")\n",
    "        for layer in self.weights:\n",
    "            print(layer)\n",
    "        print(\"\\nBiases:\" )\n",
    "        for layer in self.biases:\n",
    "            print(layer)\n",
    "        print(\"\\nActivations:\")\n",
    "        for layer in self.activations:\n",
    "            print(layer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3 5]\n",
      " [2 4 6]]\n",
      "[[7]\n",
      " [8]]\n",
      "\n",
      "\n",
      "[[10 12 14]\n",
      " [20 21 22]]\n",
      "[[18]\n",
      " [23]]\n",
      "\n",
      "\n",
      "[[ 7]\n",
      " [ 8]\n",
      " [18]\n",
      " [23]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TEST BATCH PARTITIONING\n",
    "\"\"\"\n",
    "batch_size = 2;\n",
    "training_examples = np.matrix('1, 3, 5, 7; 2, 4, 6, 8 ; 10, 12, 14, 18; 20, 21, 22, 23');\n",
    "inputs = training_examples[:, :-1];\n",
    "outputs = training_examples[:,-1];\n",
    "for batch in range(len(inputs))[0 :: batch_size]:\n",
    "    X = np.matrix(inputs[batch : batch + batch_size, :])\n",
    "    Y = np.matrix(outputs[batch: batch + batch_size])\n",
    "    print(X)\n",
    "    print(Y)\n",
    "    print(\"\\n\")\n",
    "\n",
    "print(outputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: \n",
      "[[ array([[ 1.85701115,  1.05016641, -0.3568261 ],\n",
      "       [-0.10482871,  0.60547991, -1.09448532],\n",
      "       [ 1.1293635 , -2.21308706,  0.54724683],\n",
      "       [ 2.77517515,  0.610776  ,  0.74211752],\n",
      "       [ 2.11527701, -0.35603168,  2.10814426]])\n",
      "  array([[ 1.12762617, -0.78380229,  1.10721175, -1.32314976, -0.44318806],\n",
      "       [ 1.75516116,  0.31483771,  1.45535841, -0.63231052,  0.02339723]])]]\n",
      "\n",
      "Biases:\n",
      "[[array([-1.16950126, -1.42211871,  0.57253225,  1.1786072 ,  0.28252318])\n",
      "  array([-0.7806755 ,  1.05819738])]]\n",
      "\n",
      "Activations:\n",
      "[[array([ 0.,  0.,  0.,  0.,  0.]) array([ 0.,  0.])]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TEST NETWORK CREATION\n",
    "\"\"\"\n",
    "test = Simple_NN([3, 5, 2]);\n",
    "test.print_network();"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
