{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple_NN\n",
    "Class to create a simple neural network\n",
    "    Learning Algorithm: stochastic gradient descent with backpropagation\n",
    "    Activation Function: sigmoid\n",
    "    Cost Function: MSE\n",
    "\"\"\"\n",
    "class Simple_NN(object):\n",
    "    \"\"\" \n",
    "    INITIALIZE THE NETWORK\n",
    "    \"\"\"\n",
    "    def __init__(self, layers, activation_function=\"sigmoid\", cost_function=\"MSE\"):\n",
    "        \"\"\"\n",
    "        self.layers is a list of numbers where the ith number how many neurons are in\n",
    "        the ith layer of the network.\n",
    "        \"\"\"\n",
    "        self.layers = layers;\n",
    "        self.num_layers = len(layers);\n",
    "        \n",
    "        \"\"\"\n",
    "        self.weights[Layer - 1, input_neuron, output_neuron] = \n",
    "                                            List of weight matrices for each layer.                      \n",
    "        self.biases[Layer - 1, neuron] = \n",
    "                                            List of vectors with biases for each neuron  \n",
    "        FOR EXAMPLE:\n",
    "            self.weights[layer, j, i] = weight going into the jth neuron of the lth layer\n",
    "                                    from the ith neuron of the (l-1)st layer \n",
    "            self.biases[layer, k] = bias on the kth nuron of the lth layer\n",
    "        NOTE: layer 0 is the input layer, so self.weights[0] is the weights going into layer 1\n",
    "        \"\"\"\n",
    "        self.weights = [];\n",
    "        self.biases = [];\n",
    "        self.Z = [];\n",
    "        self.activations = [];\n",
    "        # Create matrices with correct dimensions \n",
    "        for layer_num in range(1, self.num_layers):\n",
    "            self.weights.append(np.random.randn(layers[layer_num], layers[layer_num - 1]));\n",
    "            self.biases.append(np.random.randn(layers[layer_num]));\n",
    "        \"\"\"\n",
    "        self.activation = string specifying what activation function the neurons will use. \n",
    "            The options are:\n",
    "            sigmoid (default)\n",
    "        self.cost_function = string specifying hat cost function will be used for the \n",
    "            network.\n",
    "            The options are:\n",
    "            MSE (default)\n",
    "        \"\"\"\n",
    "        self.activation_function = activation_function;\n",
    "        self.cost_function = cost_function;\n",
    " \n",
    "    \"\"\"\n",
    "    TRAINING\n",
    "    Train the network using stochastic gradient descent and backpropagation.\n",
    "    Training data should be given in the following format:\n",
    "        [x11, x12, ..., x1i, y1\n",
    "         x21, x22, ..., x2i, y2\n",
    "         ...\n",
    "         xm1, xm1, ..., xmi, ym]\n",
    "    Where each row corrsponds to a training example with i data points\n",
    "    \"\"\"\n",
    "    def train(self, training_data, batch_size, num_epochs, learning_rate):\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"EPOCH: %d\" % epoch);\n",
    "            # Randomize the order of training examples\n",
    "            np.random.shuffle(training_data);\n",
    "            # Separate inputs from outputs\n",
    "            inputs = training_data[:, :-1]\n",
    "            outputs = training_data[:, -1];\n",
    "            # For each epoch, loop through each batch to use as training data\n",
    "            for batch in range(len(training_data))[::batch_size]:\n",
    "                # For each batch, we calculate activations and use the backpropagation\n",
    "                # algorithm to change the weights and biases using gradient descent\n",
    "                self.Z = [];\n",
    "                self.activations = [];\n",
    "                # Create matrix out of all training inputs in the batch\n",
    "                # If the first layer of the network has k neurons, and each training\n",
    "                # example has i data points, then weights will be a kxi matrix \n",
    "                # so Wx_j = kx1 vector.\n",
    "                # To apply W to all input vectors, we can multiply WX where\n",
    "                # X is the ixm matrix containing all m training examples as columns\n",
    "                X = inputs[batch : batch + batch_size];\n",
    "                X = np.transpose(X);\n",
    "                Y = outputs[batch : batch + batch_size];\n",
    "\n",
    "                # *** DEBUGGING  ***\n",
    "                print('Batch #%d' % batch);\n",
    "                print('X');\n",
    "                print(X);\n",
    "                print('Y')\n",
    "                print(Y);\n",
    "                \n",
    "                # FEEDFORWARD\n",
    "                \"\"\"\n",
    "                self.Z[layer, neuron, training_example] = \n",
    "                                            List of vectors with weighted inputs to the neurons\n",
    "                self.activations[layer, neuron, training_Example] = \n",
    "                                            List of vectors with activations for each neuron\n",
    "                \"\"\"\n",
    "                # Calclate outputs going forwards through the network\n",
    "                for layer in range(self.num_layers - 1):\n",
    "                    if layer == 0:\n",
    "                        # Feed inputs to the network\n",
    "                        prev_activations = X;\n",
    "                    else:\n",
    "                        prev_activations = self.activations[layer - 1];\n",
    "                    # Bias matrix where each column is a copy of the bias vector is needed\n",
    "                    # to add bias terms for each training example. \n",
    "                    one_vector = np.ones(batch_size);\n",
    "                    bias_matrix = np.outer(self.biases[layer], one_vector);\n",
    "                    self.Z.append(np.dot(self.weights[layer], prev_activations) + bias_matrix);\n",
    "                    self.activations.append(self.activation(self.Z[layer]));\n",
    "                # BACKPROPAGATION\n",
    "                # Calculate output error matrix so the [i, j]th entry contains the\n",
    "                # error for the ith neuron in the output layer for the jth training\n",
    "                # example\n",
    "                output_error = np.multiply(\n",
    "                                    self.cost_derivative(self.activations[-1], Y),\n",
    "                                    self.activation_derivative(self.Z[-1]));\n",
    "                # Backpropogate: we create the errors matrix which is indexed\n",
    "                # in the form errors[layer, neuron, training_example]\n",
    "                errors = [output_error];\n",
    "                for layer in reversed(range(self.num_layers - 2)):\n",
    "                    # For each layer, calculate errors in previous layer\n",
    "                    previous_errors = np.multiply(\n",
    "                                        np.dot(\n",
    "                                            np.transpose(self.weights[layer + 1]),\n",
    "                                            errors[0]), \n",
    "                                        self.activation_derivative(self.Z[-layer])); \n",
    "                    # Add previous errors to the beginning of the error matrix list\n",
    "                    errors.insert(0, previous_errors);\n",
    "                # Calculate gradint of cost function\n",
    "                gradC_b = [];\n",
    "                gradC_w = [];\n",
    "                for layer in range(self.num_layers - 1):\n",
    "                    # Gradient of cost wrt biases for a layer is just the\n",
    "                    # vector of errors for that layer\n",
    "                    gradC_b.append(np.average(errors[layer],1));\n",
    "                    \"\"\"\n",
    "                    sum_of_weights[layer, j, k] will contain the partial derivative \n",
    "                    of cost wrt the weight from the kth neuron in layer - 1 to the jth\n",
    "                    neuron in layer summed over all training examples. That is,\n",
    "                    sum_of_weightes[layer, j, k] = [sum over training examples dC/dw_j,k]\n",
    "                    \"\"\"\n",
    "                    if (layer == 0):\n",
    "                        prev_activations = X;\n",
    "                    else:\n",
    "                        prev_activations = self.activations[layer - 1];\n",
    "                    sum_of_weights = np.dot(\n",
    "                                            errors[layer],\n",
    "                                            np.transpose(prev_activations));\n",
    "                    gradC_w.append((1 / batch_size) * sum_of_weights);               \n",
    "                \n",
    "                # DEBUGGING\n",
    "                print(\"\\nGradient wrt biases: \")\n",
    "                print(gradC_b);\n",
    "                print(\"\\nGradient wrt weights: \")\n",
    "                print(gradC_w);\n",
    "                \n",
    "                # Gradient Descent\n",
    "                \"\"\"\n",
    "                At this point, gradC_b is of the form\n",
    "                [layer, neuron] = average gradient for that neuron over all\n",
    "                    training examples in the batch\n",
    "                and gradC_w is of the form [layer, input_neuron, output_neuron], \n",
    "                    also averaged over all traininge examples in the batch\n",
    "                \"\"\"\n",
    "                for layer in range(self.num_layers - 1):\n",
    "                    self.biases[layer] -= learning_rate * gradC_b[layer];\n",
    "                    self.weights[layer] -= learning_rate * gradC_w[layer];  \n",
    "                \n",
    "                \n",
    "    \"\"\" \n",
    "    ACTIVATION FUNCTION\n",
    "    For this network, we use the sigmoid function to calculate neuron activation\n",
    "    In general, we assume the input z will be a matrix where the [i,j]th entry is the\n",
    "    ith neuron in the jth training example\n",
    "    \"\"\"      \n",
    "    def activation(self, z):\n",
    "        if (self.activation_function == \"sigmoid\"):\n",
    "            return 1.0 / (1 + np.exp(-z));\n",
    "    \n",
    "    def activation_derivative(self, z):\n",
    "        if (self.activation_function == \"sigmoid\"):\n",
    "            return np.multiply((1 - self.activation(z)), self.activation(z));\n",
    "    \"\"\"\n",
    "    COST FUNCTION\n",
    "    We assume that activations is a 2D matrix where each column corresponds to the activations\n",
    "    for a specific training example and each row corresponds to a specific neuron\n",
    "\n",
    "    We assume for now that outputs for the network are disjoint categories, so only\n",
    "    one output neuron should fire at a time. the output_matrix function turns the output\n",
    "    vector where each entry corresponds to a training example into \n",
    "    \n",
    "    Both calculate_cost and cost_derivative return a row vector where the ith entry\n",
    "    is the cost/cost derivative for training example i\n",
    "    \"\"\"\n",
    "    def output_matrix(self, activations, outputs):\n",
    "        # output_matrix[neuron, training_example]\n",
    "        output_matrix = np.zeros((activations.shape[0], outputs.shape[0]));\n",
    "        for training_example, output in enumerate(outputs):\n",
    "            output_matrix[output, training_example] = 1;\n",
    "        return output_matrix;\n",
    "    \n",
    "    def calculate_cost(self, activations, outputs):\n",
    "        outputs_matrix = self.output_matrix(activations, outputs);\n",
    "        if (self.cost_function == \"MSE\"):\n",
    "            squared_errors = np.square(activations - output_matrix);\n",
    "            # Average sum of squared errors over each training example\n",
    "            return 0.5 * np.average(np.sum(squared_errors, 0));\n",
    "    \"\"\"\n",
    "    Derivative of cost functions with respect to activations\n",
    "    Each entry [i, j] in the resulting matrix will be the gradient of the cost function\n",
    "    with respect to the activation of the ith neuron in the jth training example\n",
    "    \"\"\"\n",
    "    def cost_derivative(self, activations, outputs):\n",
    "        output_matrix = self.output_matrix(activations, outputs);\n",
    "        if (self.cost_function == \"MSE\"):\n",
    "            return (activations - output_matrix);\n",
    "       \n",
    "    \"\"\" \n",
    "    TODO: \n",
    "        test(testing_data)\n",
    "        predict(data)\n",
    "    \"\"\"\n",
    "    def print_network(self):\n",
    "        print(\"Number of layers: \");\n",
    "        print(self.num_layers);\n",
    "        print(\"Weights: \")\n",
    "        for layer in self.weights:\n",
    "            print(layer)\n",
    "        print(\"\\nBiases:\" )\n",
    "        for layer in self.biases:\n",
    "            print(layer)\n",
    "        print(\"\\nWeighted Inputs:\")\n",
    "        for layer in self.Z:\n",
    "            print(layer)\n",
    "        print(\"\\nActivations:\")\n",
    "        for layer in self.activations:\n",
    "            print(layer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n",
      "Batch #0\n",
      "X\n",
      "[[ 1 11]\n",
      " [ 2 12]\n",
      " [ 3 13]]\n",
      "Y\n",
      "[[0]\n",
      " [1]]\n",
      "\n",
      "Gradient wrt biases: \n",
      "[matrix([[-0.004144  ],\n",
      "        [ 0.0209995 ],\n",
      "        [-0.00144362],\n",
      "        [ 0.00471528],\n",
      "        [-0.00010283],\n",
      "        [ 0.00029675]]), matrix([[-0.03533802],\n",
      "        [-0.03470577],\n",
      "        [ 0.07828954],\n",
      "        [ 0.00748207]])]\n",
      "\n",
      "Gradient wrt weights: \n",
      "[matrix([[-0.00861586, -0.01275986, -0.01690385],\n",
      "        [ 0.0209976 ,  0.0419971 ,  0.0629966 ],\n",
      "        [-0.00144362, -0.00288724, -0.00433086],\n",
      "        [ 0.00471534,  0.00943063,  0.01414591],\n",
      "        [-0.00010101, -0.00020383, -0.00030666],\n",
      "        [ 0.00029675,  0.00059349,  0.00089024]]), matrix([[ -3.24009286e-02,  -1.30294731e-02,  -8.03356125e-04,\n",
      "          -3.37070153e-02,  -3.51273862e-02,  -3.50971298e-02],\n",
      "        [ -3.25804265e-02,   1.10101859e-03,   6.78959149e-05,\n",
      "           2.84869000e-03,  -3.47229395e-02,  -3.47261252e-02],\n",
      "        [  7.29169854e-02,   8.10037048e-03,   4.99427414e-04,\n",
      "           2.09549548e-02,   7.81576569e-02,   7.81397865e-02],\n",
      "        [  6.97695740e-03,   6.21444354e-04,   3.83146659e-05,\n",
      "           1.60760790e-03,   7.47192373e-03,   7.47057738e-03]])]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (6,) doesn't match the broadcast shape (6,6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-d84e196b34d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"After training\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-bc97aaa74426>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, training_data, batch_size, num_epochs, learning_rate)\u001b[0m\n\u001b[0;32m    166\u001b[0m                 \"\"\"\n\u001b[0;32m    167\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_layers\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbiases\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgradC_b\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgradC_w\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape (6,) doesn't match the broadcast shape (6,6)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TEST NETWORK CREATION\n",
    "\"\"\"\n",
    "test = Simple_NN([3, 6, 4]);\n",
    "random_data = np.matrix('1, 2, 3, 0; 11, 12, 13, 1; 21, 22, 23, 2; 31, 32, 33, 3')\n",
    "batch_size = 2;\n",
    "num_epochs = 1;\n",
    "learning_rate = 0.5;\n",
    "test.train(random_data, batch_size, num_epochs, learning_rate);\n",
    "\n",
    "print(\"After training\");\n",
    "test.print_network();\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
