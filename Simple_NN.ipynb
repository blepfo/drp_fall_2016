{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple_NN\n",
    "Class to create a simple neural network\n",
    "    Learning Algorithm: stochastic gradient descent with backpropagation\n",
    "    Activation Function: sigmoid\n",
    "    Cost Function: MSE\n",
    "\"\"\"\n",
    "class Simple_NN(object):\n",
    "    \"\"\" \n",
    "    INITIALIZE THE NETWORK\n",
    "    \"\"\"\n",
    "    def __init__(self, layers, activation_function=\"sigmoid\", cost_function=\"MSE\"):\n",
    "        \"\"\"\n",
    "        self.layers is a list of numbers where the ith number how many neurons are in\n",
    "        the ith layer of the network.\n",
    "        \"\"\"\n",
    "        self.layers = layers;\n",
    "        self.num_layers = len(layers);\n",
    "        \n",
    "        \"\"\"\n",
    "        self.weights[Layer - 1, input_neuron, output_neuron] = \n",
    "                                            List of weight matrices for each layer.                      \n",
    "        self.biases[Layer - 1, neuron] = \n",
    "                                            List of vectors with biases for each neuron  \n",
    "        FOR EXAMPLE:\n",
    "            self.weights[layer, j, i] = weight going into the jth neuron of the lth layer\n",
    "                                    from the ith neuron of the (l-1)st layer \n",
    "            self.biases[layer, k] = bias on the kth nuron of the lth layer\n",
    "        NOTE: layer 0 is the input layer, so self.weights[0] is the weights going into layer 1\n",
    "        \"\"\"\n",
    "        self.weights = [];\n",
    "        self.biases = [];\n",
    "        self.Z = [];\n",
    "        self.activations = [];\n",
    "        # Create matrices with correct dimensions \n",
    "        for layer_num in range(1, self.num_layers):\n",
    "            self.weights.append(np.random.randn(layers[layer_num], layers[layer_num - 1]));\n",
    "            self.biases.append(np.random.randn(layers[layer_num]));\n",
    "        \"\"\"\n",
    "        self.activation = string specifying what activation function the neurons will use. \n",
    "            The options are:\n",
    "            sigmoid (default)\n",
    "        self.cost_function = string specifying hat cost function will be used for the \n",
    "            network.\n",
    "            The options are:\n",
    "            MSE (default)\n",
    "        \"\"\"\n",
    "        self.activation_function = activation_function;\n",
    "        self.cost_function = cost_function;\n",
    "        self.errors = [];\n",
    "        self.gradC_b = [];\n",
    "        self.gradC_w = [];\n",
    " \n",
    "    \"\"\"\n",
    "    TRAINING\n",
    "    Train the network using stochastic gradient descent and backpropagation.\n",
    "    Training data should be given in the following format:\n",
    "        [x11, x12, ..., x1i, y1\n",
    "         x21, x22, ..., x2i, y2\n",
    "         ...\n",
    "         xm1, xm1, ..., xmi, ym]\n",
    "    Where each row corrsponds to a training example with i data points\n",
    "    \"\"\"\n",
    "    def train(self, training_data, batch_size, num_epochs, learning_rate):\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"EPOCH: %d\" % epoch);\n",
    "            # Randomize the order of training examples\n",
    "            np.random.shuffle(training_data);\n",
    "            # Separate inputs from outputs\n",
    "            inputs = training_data[:, :-1]\n",
    "            outputs = training_data[:, -1];\n",
    "            # For each epoch, loop through each batch to use as training data\n",
    "            for batch in range(len(training_data))[::batch_size]:\n",
    "                \"\"\"\n",
    "                For each batch, we calculate activations and use the backpropagation\n",
    "                algorithm to change the weights and biases using gradient descent\n",
    "                Create matrix out of all training inputs in the batch\n",
    "                To apply W to all input vectors, we can multiply WX where\n",
    "                X is the ixm matrix containing all m training examples as columns\n",
    "                \"\"\"\n",
    "                X = inputs[batch : batch + batch_size];\n",
    "                X = np.transpose(X);\n",
    "                Y = outputs[batch : batch + batch_size];\n",
    "                # FEEDFORWARD\n",
    "                self.evaluate(X);\n",
    "                # BACKPROPAGATION\n",
    "                self.backpropagate(X, Y);\n",
    "                # GRADIENT DESCENT\n",
    "                self.grad_descent();\n",
    "    \n",
    "    \"\"\"\n",
    "    EVALUATE\n",
    "    Takes an input vector X and computes the output of the network\n",
    "    \"\"\"\n",
    "    def evaluate(self, X):\n",
    "        \"\"\"\n",
    "        self.Z[layer, neuron, training_example] = \n",
    "                        List of vectors with weighted inputs to the neurons\n",
    "        self.activations[layer, neuron, training_Example] = \n",
    "                        List of vectors with activations for each neuron\n",
    "        \"\"\"\n",
    "        self.Z = [];\n",
    "        self.activations = [];\n",
    "        # Calclate outputs going forwards through the network\n",
    "        for layer in range(self.num_layers - 1):\n",
    "            if layer == 0:\n",
    "                # Feed inputs to the network\n",
    "                prev_activations = X;\n",
    "            else:\n",
    "                prev_activations = self.activations[layer - 1];\n",
    "            # Bias matrix where each column is a copy of the bias vector is needed\n",
    "            # to add bias terms for each training example. \n",
    "            one_vector = np.ones(batch_size);\n",
    "            bias_matrix = np.outer(self.biases[layer], one_vector);  \n",
    "            self.Z.append(np.dot(self.weights[layer], prev_activations) + bias_matrix);\n",
    "            self.activations.append(self.activation(self.Z[layer]));\n",
    "        \n",
    "    \"\"\"\n",
    "    BACKPROPAGATE\n",
    "    Using input vector X and output vector Y, calculates the following instance variables:\n",
    "        self.errors[layer, neuron, training_example] - error in each neuron\n",
    "        self.gradC_b[layer] - average gradient wrt biases over all training examples\n",
    "        self.gradC_w[layer, input_neuron, output_neuron] - average gradient wrt weights\n",
    "    \"\"\"\n",
    "    def backpropagate(self, X, Y):\n",
    "        # Calculate output error matrix so the [i, j]th entry contains the\n",
    "        # error for the ith neuron in the output layer for the jth training\n",
    "        # example\n",
    "        output_error = np.multiply(\n",
    "                             self.cost_derivative(self.activations[-1], Y),\n",
    "                             self.activation_derivative(self.Z[-1]));\n",
    "        # Backpropogate: we create the errors matrix which is indexed\n",
    "        # in the form errors[layer, neuron, training_example]\n",
    "        self.errors = [output_error];\n",
    "        for layer in reversed(range(self.num_layers - 2)):\n",
    "            # For each layer, calculate errors in previous layer\n",
    "            previous_errors = np.multiply(\n",
    "                                np.dot(\n",
    "                                    np.transpose(self.weights[layer + 1]),\n",
    "                                    self.errors[0]), \n",
    "                                self.activation_derivative(self.Z[-layer])); \n",
    "            # Add previous errors to the beginning of the error matrix list\n",
    "            self.errors.insert(0, previous_errors);\n",
    "        # Calculate gradients of cost function\n",
    "        self.gradC_b = [];\n",
    "        self.gradC_w = [];\n",
    "        for layer in range(self.num_layers - 1):\n",
    "            \"\"\"\n",
    "            gradC_b[layer, neuron] = \n",
    "            Gradient of cost wrt biases for a layer is just the\n",
    "            vector of errors for that layer.\n",
    "            When we compute the average over all training examples using \n",
    "            np.average(matrix, 1), we get a column vector. In order to\n",
    "            correctly subtract this from the row-vector of biases, \n",
    "            we transpose the gradient vectors.\n",
    "            \"\"\"\n",
    "            self.gradC_b.append(np.average(self.errors[layer],1));\n",
    "            \"\"\"\n",
    "            sum_of_weights[layer, j, k] will contain the partial derivative \n",
    "            of cost wrt the weight from the kth neuron in layer - 1 to the jth\n",
    "            neuron in layer summed over all training examples. That is,\n",
    "            sum_of_weightes[layer, j, k] = [sum over training examples dC/dw_j,k]\n",
    "            \"\"\"\n",
    "            if (layer == 0):\n",
    "               prev_activations = X;\n",
    "            else:\n",
    "               prev_activations = self.activations[layer - 1];\n",
    "            sum_of_weights = np.dot(\n",
    "                                    self.errors[layer],\n",
    "                                    np.transpose(prev_activations));\n",
    "            \"\"\"\n",
    "            gradC_w [layer, input_neuron, output_neuron] is also \n",
    "                averaged over all traininge examples in the batch\n",
    "            \"\"\"\n",
    "            self.gradC_w.append((1 / batch_size) * sum_of_weights);\n",
    "    \n",
    "    \"\"\"\n",
    "    GRADIENT DESCENT\n",
    "    Use the gradients computed in backpropagate() to update the weights and\n",
    "    biases of the network.\n",
    "    \"\"\"\n",
    "    def grad_descent(self):\n",
    "        for layer in range(self.num_layers - 1):\n",
    "            self.biases[layer] = np.subtract(self.biases[layer],\n",
    "                        np.transpose(np.multiply(learning_rate, self.gradC_b[layer])));\n",
    "            self.weights[layer] = np.subtract(self.weights[layer],\n",
    "                        np.multiply(learning_rate, self.gradC_w[layer]));\n",
    "        \n",
    "    \"\"\" \n",
    "    ACTIVATION FUNCTION\n",
    "    For this network, we use the sigmoid function to calculate neuron activation\n",
    "    In general, we assume the input z will be a matrix where the [i,j]th entry is the\n",
    "    ith neuron in the jth training example\n",
    "    \"\"\"      \n",
    "    def activation(self, z):\n",
    "        if (self.activation_function == \"sigmoid\"):\n",
    "            return 1.0 / (1 + np.exp(-z));\n",
    "    \n",
    "    def activation_derivative(self, z):\n",
    "        if (self.activation_function == \"sigmoid\"):\n",
    "            return np.multiply((1 - self.activation(z)), self.activation(z));\n",
    "    \"\"\"\n",
    "    COST FUNCTION\n",
    "    We assume that activations is a 2D matrix where each column corresponds to the activations\n",
    "    for a specific training example and each row corresponds to a specific neuron\n",
    "\n",
    "    We assume for now that outputs for the network are disjoint categories, so only\n",
    "    one output neuron should fire at a time. the output_matrix function turns the output\n",
    "    vector where each entry corresponds to a training example into \n",
    "    \n",
    "    Both calculate_cost and cost_derivative return a row vector where the ith entry\n",
    "    is the cost/cost derivative for training example i\n",
    "    \"\"\"\n",
    "    def output_matrix(self, activations, outputs):\n",
    "        # output_matrix[neuron, training_example]\n",
    "        output_matrix = np.zeros((activations.shape[0], outputs.shape[0]));\n",
    "        for training_example, output in enumerate(outputs):\n",
    "            output_matrix[output, training_example] = 1;\n",
    "        return output_matrix;\n",
    "    \n",
    "    def calculate_cost(self, activations, outputs):\n",
    "        outputs_matrix = self.output_matrix(activations, outputs);\n",
    "        if (self.cost_function == \"MSE\"):\n",
    "            squared_errors = np.square(activations - output_matrix);\n",
    "            # Average sum of squared errors over each training example\n",
    "            return 0.5 * np.average(np.sum(squared_errors, 0));\n",
    "    \"\"\"\n",
    "    Derivative of cost functions with respect to activations\n",
    "    Each entry [i, j] in the resulting matrix will be the gradient of the cost function\n",
    "    with respect to the activation of the ith neuron in the jth training example\n",
    "    \"\"\"\n",
    "    def cost_derivative(self, activations, outputs):\n",
    "        output_matrix = self.output_matrix(activations, outputs);\n",
    "        if (self.cost_function == \"MSE\"):\n",
    "            return (activations - output_matrix);\n",
    "       \n",
    "    \"\"\" \n",
    "    TODO: \n",
    "        predict(data)\n",
    "        test(testing_data)\n",
    "    \"\"\"\n",
    "    \n",
    "    def print_network(self):\n",
    "        print(\"Number of layers: \");\n",
    "        print(self.num_layers);\n",
    "        print(\"Weights: \")\n",
    "        for layer in self.weights:\n",
    "            print(layer)\n",
    "        print(\"\\nBiases:\" )\n",
    "        for layer in self.biases:\n",
    "            print(layer)\n",
    "        print(\"\\nWeighted Inputs:\")\n",
    "        for layer in self.Z:\n",
    "            print(layer)\n",
    "        print(\"\\nActivations:\")\n",
    "        for layer in self.activations:\n",
    "            print(layer);\n",
    "        print(\"\\nErrors:\")\n",
    "        for layer in self.errors:\n",
    "            print(layer)\n",
    "        print(\"\\nBias Gradients:\")\n",
    "        for layer in self.gradC_b:\n",
    "            print(layer)\n",
    "        print(\"\\nWeight Gradients\")\n",
    "        for layer in self.gradC_w:\n",
    "            print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: \n",
      "4\n",
      "Weights: \n",
      "[[-1.61771958  0.06104969  0.00344231]\n",
      " [-2.47739367  0.37182968  0.87340276]\n",
      " [ 1.31312222 -0.86031807  0.75642466]\n",
      " [ 1.73453476 -0.81626848  0.28220816]\n",
      " [-1.64962275  0.60686692 -0.76529831]\n",
      " [-0.83879163  0.04232698 -0.61938099]]\n",
      "[[ 1.07287943 -0.5555664  -1.68427828  0.38250352 -0.50628847  1.21515235]\n",
      " [ 1.36542498 -0.66913617  0.38599839 -0.86535038  1.67083251 -0.74826367]\n",
      " [ 0.33547946 -0.12314581  0.23082536  0.09084603  0.9970974   0.61225936]\n",
      " [ 0.78368019  0.76315456  0.73285797 -0.15127685 -1.66275395  0.06463412]\n",
      " [ 1.30177859  2.63977935  1.00810222  0.26421393  0.47419188  1.10349807]\n",
      " [ 0.29716596  0.44379007  0.36665464  0.50601617 -0.60312526  1.54606957]\n",
      " [ 0.8879631  -0.42560716  0.11350218 -1.0100634   1.04813219  0.07360259]]\n",
      "[[ 2.6322296  -0.37860545  2.09499659  1.43228942 -0.53062994 -0.45205549\n",
      "  -1.14821708]\n",
      " [ 0.78884041  0.24146051  1.14077844  0.94337406 -1.02239426 -0.5060551\n",
      "  -0.87619687]\n",
      " [ 0.9593506   0.47170108 -0.01304444 -0.47968918  0.06217155  0.41852104\n",
      "  -2.76768848]\n",
      " [-1.17349548 -0.74809925  0.82486628  0.72003107 -0.18163446 -0.87020144\n",
      "  -0.8019615 ]]\n",
      "\n",
      "Biases:\n",
      "[-0.23003486  0.43125713  0.59534966  0.67938928  0.68299586 -0.393861  ]\n",
      "[-1.87756203  0.17614404 -2.67956     0.85928803 -0.92049582 -1.24043574\n",
      " -0.88850698]\n",
      "[-0.77183489 -0.75125885  1.31994965  0.32404687]\n",
      "\n",
      "Weighted Inputs:\n",
      "\n",
      "Activations:\n",
      "\n",
      "Errors:\n",
      "\n",
      "Bias Gradients:\n",
      "\n",
      "Weight Gradients\n",
      "\n",
      "\n",
      "BEGIN TRAINING\n",
      "\n",
      "EPOCH: 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (7,2) (4,2) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-fde77cef1575>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\\nBEGIN TRAINING\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"After training\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-1eb9c00b2506>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, training_data, batch_size, num_epochs, learning_rate)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[1;31m# BACKPROPAGATION\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackpropagate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m                 \u001b[1;31m# GRADIENT DESCENT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-1eb9c00b2506>\u001b[0m in \u001b[0;36mbackpropagate\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    139\u001b[0m                                     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlayer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m                                     self.errors[0]), \n\u001b[1;32m--> 141\u001b[1;33m                                 self.activation_derivative(self.Z[-layer])); \n\u001b[0m\u001b[0;32m    142\u001b[0m             \u001b[1;31m# Add previous errors to the beginning of the error matrix list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_errors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (7,2) (4,2) "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TEST NETWORK CREATION\n",
    "\"\"\"\n",
    "test = Simple_NN([3, 6, 7, 4]);\n",
    "random_data = np.matrix('1, 2, 3, 0; 11, 12, 13, 1; 21, 22, 23, 2; 31, 32, 33, 3')\n",
    "batch_size = 2;\n",
    "num_epochs = 3;\n",
    "learning_rate = 0.5;\n",
    "test.print_network();\n",
    "\n",
    "print(\"\\n\\nBEGIN TRAINING\\n\")\n",
    "\n",
    "test.train(random_data, batch_size, num_epochs, learning_rate);\n",
    "\n",
    "print(\"After training\");\n",
    "test.print_network();\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
