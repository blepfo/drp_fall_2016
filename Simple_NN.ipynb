{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np # For matrix/vector operations\n",
    "import gzip        # For un-zipping the MNIST data\n",
    "import pickle      # For de-serializing the MNIST data\n",
    "import matplotlib.pyplot as pyplot # For graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We use the serialized and zipped copy of the MNIST data available at Michael Nielson's \n",
    "repository:\n",
    "https://github.com/mnielsen/neural-networks-and-deep-learning\n",
    "\"\"\"\n",
    "# Unzip the MNIST Data\n",
    "with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "    # Unpickle (de-serialize) the\n",
    "    mnist_data = pickle.load(f, encoding='bytes');\n",
    "    \n",
    "\"\"\"\n",
    "mnist_data = 3-tuple containing (mnist_training, mnist_validation, mnist_testing)\n",
    "    mnist_training = 2-tuple containing (training_data, training_labels)\n",
    "        training_data = [50,000, 784] ndarray of data for each training example\n",
    "        training_labels = [50,000] ndarray of labels for the training_data\n",
    "    mnist_validation = 2-tuple containing (validation_data, validation_labels)\n",
    "        validation_data = [10,000, 784] ndarray of data for each validation image\n",
    "        validation_labels = [10,000] ndarray of labels for the training_data\n",
    "    mnist_testing = 2-tuple containing (testing_data, testing_labels)\n",
    "        testing_data = [10,000, 784] ndarray of data for each testing image\n",
    "        testing_labels = [10,000] ndarray of labels for testing data\n",
    "\"\"\"    \n",
    "training_data = mnist_data[0][0];\n",
    "training_labels = mnist_data[0][1];\n",
    "validation_data = mnist_data[1][0];\n",
    "validation_labels = mnist_data[1][1];\n",
    "testing_data = mnist_data[2][0];\n",
    "testing_labels = mnist_data[2][1];\n",
    "# Concatenate lables to end of each training example to work with the implementation\n",
    "# of the train() function\n",
    "# Now training_examples, validation_examples, and testing_examples are all \n",
    "# [50,000, 785] or [10,000, 785] arrays where the last column contains the labels\n",
    "training_examples = np.column_stack((training_data, training_labels));\n",
    "validation_examples = np.column_stack((validation_data, validation_labels));\n",
    "testing_examples = np.column_stack((testing_data, testing_labels));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple_NN\n",
    "Class to create a simple neural network\n",
    "    Learning Algorithm: stochastic gradient descent with backpropagation\n",
    "    Activation Function: sigmoid\n",
    "    Cost Function: MSE\n",
    "\"\"\"\n",
    "class Simple_NN(object):\n",
    "    \"\"\" \n",
    "    INITIALIZE THE NETWORK\n",
    "    \"\"\"\n",
    "    def __init__(self, layers, activation_function=\"sigmoid\", cost_function=\"MSE\"):\n",
    "        \"\"\"\n",
    "        self.layers is a list of numbers where the ith number how many neurons are in\n",
    "        the ith layer of the network.\n",
    "        \"\"\"\n",
    "        self.layers = layers;\n",
    "        self.num_layers = len(layers);\n",
    "        \n",
    "        \"\"\"\n",
    "        self.weights[Layer - 1, input_neuron, output_neuron] = \n",
    "                                            List of weight matrices for each layer.                      \n",
    "        self.biases[Layer - 1, neuron] = \n",
    "                                            List of vectors with biases for each neuron  \n",
    "        FOR EXAMPLE:\n",
    "            self.weights[layer, j, i] = weight going into the jth neuron of the lth layer\n",
    "                                    from the ith neuron of the (l-1)st layer \n",
    "            self.biases[layer, k] = bias on the kth nuron of the lth layer\n",
    "        NOTE: layer 0 is the input layer, so self.weights[0] is the weights going into layer 1\n",
    "        \"\"\"\n",
    "        self.weights = [];\n",
    "        self.biases = [];\n",
    "        self.Z = [];\n",
    "        self.activations = [];\n",
    "        # Create matrices with correct dimensions \n",
    "        for layer_num in range(1, self.num_layers):\n",
    "            self.weights.append(np.random.randn(layers[layer_num], layers[layer_num - 1]));\n",
    "            self.biases.append(np.random.randn(layers[layer_num]));\n",
    "        \"\"\"\n",
    "        self.activation = string specifying what activation function the neurons will use. \n",
    "            The options are:\n",
    "            sigmoid (default)\n",
    "        self.cost_function = string specifying hat cost function will be used for the \n",
    "            network.\n",
    "            The options are:\n",
    "            MSE (default)\n",
    "        \"\"\"\n",
    "        self.activation_function = activation_function;\n",
    "        self.cost_function = cost_function;\n",
    "        self.errors = [];\n",
    "        self.gradC_b = [];\n",
    "        self.gradC_w = [];\n",
    " \n",
    "    \"\"\"\n",
    "    TRAINING\n",
    "    Train the network using stochastic gradient descent and backpropagation.\n",
    "    Training data should be given in the following format:\n",
    "        [x11, x12, ..., x1i, y1\n",
    "         x21, x22, ..., x2i, y2\n",
    "         ...\n",
    "         xm1, xm1, ..., xmi, ym]\n",
    "    Where each row corrsponds to a training example with i data points\n",
    "    \"\"\"\n",
    "    def train(self, training_data, testing_data, batch_size, num_epochs, learning_rate):\n",
    "        for epoch in range(num_epochs):\n",
    "            # Randomize the order of training examples\n",
    "            np.random.shuffle(training_data);\n",
    "            # Separate inputs from outputs\n",
    "            inputs = training_data[:, :-1];\n",
    "            outputs = training_data[:, -1];\n",
    "            testing_inputs = testing_data[:, :-1];\n",
    "            testing_labels = testing_data[:, -1];\n",
    "            # For each epoch, loop through each batch to use as training data\n",
    "            for batch in range(len(training_data))[::batch_size]:\n",
    "                \"\"\"\n",
    "                For each batch, we calculate activations and use the backpropagation\n",
    "                algorithm to change the weights and biases using gradient descent\n",
    "                Create matrix out of all training inputs in the batch\n",
    "                To apply W to all input vectors, we can multiply WX where\n",
    "                X is the ixm matrix containing all m training examples as columns\n",
    "                \"\"\"\n",
    "                X = inputs[batch : batch + batch_size];\n",
    "                Y = outputs[batch : batch + batch_size];\n",
    "                X = np.transpose(X);\n",
    "                # FEEDFORWARD\n",
    "                self.evaluate(X, training=True);\n",
    "                # BACKPROPAGATION\n",
    "                self.backpropagate(X, Y);\n",
    "                # GRADIENT DESCENT\n",
    "                self.grad_descent();\n",
    "            # For each epoch, compute number of correct outputs out of testing data\n",
    "            print(\"Epoch %d: %s\" % \\\n",
    "                  ((epoch + 1), self.test(testing_inputs, testing_labels)));\n",
    "    \"\"\"\n",
    "    EVALUATE\n",
    "    Takes an input column vector X and computes the output of the network\n",
    "    \"\"\"\n",
    "    def evaluate(self, X, training=False):\n",
    "        \"\"\"\n",
    "        self.Z[layer, neuron, training_example] = \n",
    "                        List of vectors with weighted inputs to the neurons\n",
    "        self.activations[layer, neuron, training_Example] = \n",
    "                        List of vectors with activations for each neuron\n",
    "        \"\"\"\n",
    "        self.Z = [];\n",
    "        self.activations = [];\n",
    "        # Calclate outputs going forwards through the network\n",
    "        for layer in range(self.num_layers - 1):\n",
    "            if layer == 0:\n",
    "                # Feed inputs to the network\n",
    "                prev_activations = X;\n",
    "            else:\n",
    "                prev_activations = self.activations[layer - 1];\n",
    "            # If training, X will be a 2D matrix [input, training_example]. In this case,\n",
    "            # bias_matrix where each column is a copy of the bias vector is needed\n",
    "            # to add bias terms for each training example.\n",
    "            if (training):\n",
    "                one_vector = np.ones(np.shape(X)[1]);\n",
    "                bias_matrix = np.outer(self.biases[layer], one_vector);\n",
    "            else:\n",
    "                # When evaluating single training example, we want a 1D bias vector\n",
    "                bias_matrix = self.biases[layer];\n",
    "            self.Z.append(np.dot(self.weights[layer], prev_activations) + bias_matrix);\n",
    "            self.activations.append(self.activation(self.Z[layer]));\n",
    "        \n",
    "    \"\"\"\n",
    "    BACKPROPAGATE\n",
    "    Using input vector X and output vector Y, calculates the following instance variables:\n",
    "        self.errors[layer, neuron, training_example] - error in each neuron\n",
    "        self.gradC_b[layer] - average gradient wrt biases over all training examples\n",
    "        self.gradC_w[layer, input_neuron, output_neuron] - average gradient wrt weights\n",
    "    \"\"\"\n",
    "    def backpropagate(self, X, Y):\n",
    "        # Calculate output error matrix so the [i, j]th entry contains the\n",
    "        # error for the ith neuron in the output layer for the jth training\n",
    "        # example\n",
    "        output_error = np.multiply(\n",
    "                             self.cost_derivative(self.activations[-1], Y),\n",
    "                             self.activation_derivative(self.Z[-1]));\n",
    "        # Backpropogate: we create the errors matrix which is indexed\n",
    "        # in the form errors[layer, neuron, training_example]\n",
    "        self.errors = [output_error];\n",
    "        # Note that in the loop, we use negative subscripts to go through the \n",
    "        # layers from output towards input. We therefore start at layer [-2], the\n",
    "        # second to last layer\n",
    "        for layer in range(2, self.num_layers):               \n",
    "            # For each layer, calculate errors in previous layer\n",
    "            previous_errors = np.multiply(\n",
    "                                np.dot(\n",
    "                                    np.transpose(self.weights[-layer + 1]),\n",
    "                                    self.errors[0]), \n",
    "                                self.activation_derivative(self.Z[-layer])); \n",
    "            # Add previous errors to the beginning of the error matrix list\n",
    "            self.errors.insert(0, previous_errors);\n",
    "        # Calculate gradients of cost function\n",
    "        self.gradC_b = [];\n",
    "        self.gradC_w = [];\n",
    "        for layer in range(self.num_layers - 1):\n",
    "            \"\"\"\n",
    "            gradC_b[layer, neuron] = \n",
    "            Gradient of cost wrt biases for a layer is just the\n",
    "            vector of errors for that layer.\n",
    "            When we compute the average over all training examples using \n",
    "            np.average(matrix, 1), we get a column vector. In order to\n",
    "            correctly subtract this from the row-vector of biases, \n",
    "            we transpose the gradient vectors.\n",
    "            \"\"\"\n",
    "            self.gradC_b.append(np.average(self.errors[layer],1));\n",
    "            \"\"\"\n",
    "            sum_of_weights[layer, j, k] will contain the partial derivative \n",
    "            of cost wrt the weight from the kth neuron in layer - 1 to the jth\n",
    "            neuron in layer summed over all training examples. That is,\n",
    "            sum_of_weightes[layer, j, k] = [sum over training examples dC/dw_j,k]\n",
    "            \"\"\"\n",
    "            if (layer == 0):\n",
    "               prev_activations = X;\n",
    "            else:\n",
    "               prev_activations = self.activations[layer - 1];\n",
    "            sum_of_weights = np.dot(\n",
    "                                    self.errors[layer],\n",
    "                                    np.transpose(prev_activations));\n",
    "            \"\"\"\n",
    "            gradC_w [layer, input_neuron, output_neuron] is also \n",
    "                averaged over all traininge examples in the batch\n",
    "            \"\"\"\n",
    "            self.gradC_w.append((1 / batch_size) * sum_of_weights);\n",
    "    \n",
    "    \"\"\"\n",
    "    GRADIENT DESCENT\n",
    "    Use the gradients computed in backpropagate() to update the weights and\n",
    "    biases of the network.\n",
    "    \"\"\"\n",
    "    def grad_descent(self):\n",
    "        for layer in range(self.num_layers - 1):\n",
    "            self.biases[layer] = np.subtract(self.biases[layer],\n",
    "                        np.transpose(np.multiply(learning_rate, self.gradC_b[layer])));\n",
    "            self.weights[layer] = np.subtract(self.weights[layer],\n",
    "                        np.multiply(learning_rate, self.gradC_w[layer]));\n",
    "        \n",
    "    \"\"\" \n",
    "    ACTIVATION FUNCTION\n",
    "    For this network, we use the sigmoid function to calculate neuron activation\n",
    "    In general, we assume the input z will be a matrix where the [i,j]th entry is the\n",
    "    ith neuron in the jth training example\n",
    "    \"\"\"      \n",
    "    def activation(self, z):\n",
    "        if (self.activation_function == \"sigmoid\"):\n",
    "            return 1.0 / (1 + np.exp(-z));\n",
    "    \n",
    "    def activation_derivative(self, z):\n",
    "        if (self.activation_function == \"sigmoid\"):\n",
    "            return np.multiply((1 - self.activation(z)), self.activation(z));\n",
    "    \"\"\"\n",
    "    COST FUNCTION\n",
    "    We assume that activations is a 2D matrix where each column corresponds to the activations\n",
    "    for a specific training example and each row corresponds to a specific neuron\n",
    "\n",
    "    We assume for now that outputs for the network are disjoint categories, so only\n",
    "    one output neuron should fire at a time. The output_matrix function turns the output\n",
    "    vector where each entry corresponds to a training example into a matrix where each\n",
    "    column corresponds to a training example, with only one nonzero entry per column\n",
    "    corresponding to the output for that example\n",
    "    \n",
    "    Both calculate_cost and cost_derivative return a row vector where the ith entry\n",
    "    is the cost/cost derivative for training example i\n",
    "    \"\"\"\n",
    "    def output_matrix(self, activations, outputs):\n",
    "        # output_matrix[neuron, training_example]\n",
    "        output_matrix = np.zeros((activations.shape[0], outputs.shape[0]));        \n",
    "        for training_example, output in enumerate(outputs): \n",
    "            output_matrix[int(output) - 1, training_example] = 1;\n",
    "        return output_matrix;\n",
    "\n",
    "    def calculate_cost(self, activations, outputs):\n",
    "        outputs_matrix = self.output_matrix(activations, outputs);\n",
    "        if (self.cost_function == \"MSE\"):\n",
    "            squared_errors = np.square(activations - output_matrix);\n",
    "            # Average sum of squared errors over each training example\n",
    "            return 0.5 * np.average(np.sum(squared_errors, 0));\n",
    "    \"\"\"\n",
    "    Derivative of cost functions with respect to activations\n",
    "    Each entry [i, j] in the resulting matrix will be the gradient of the cost function\n",
    "    with respect to the activation of the ith neuron in the jth training example\n",
    "    \"\"\"\n",
    "    def cost_derivative(self, activations, outputs):\n",
    "        output_matrix = self.output_matrix(activations, outputs);\n",
    "        if (self.cost_function == \"MSE\"):\n",
    "            return (activations - output_matrix);\n",
    "        \n",
    "    \"\"\"\n",
    "    PREDICT\n",
    "    predict(x) - Given a data vector x, returns an integer for which class the network \n",
    "        predicts x belongs to. That is, the integer returned is the neuron in the \n",
    "        output layer with the maximum activation, indexed starting at 0.\n",
    "    \"\"\"\n",
    "    def predict(self, x):\n",
    "        self.evaluate(x);\n",
    "        return (np.argmax(self.activations[-1]) + 1) % 10;\n",
    "    \n",
    "    \"\"\"\n",
    "    test(testing_data, testing_labels) - predicts class for all data in \n",
    "        testing_data and computes accuracy as a percentage using \n",
    "    \"\"\"\n",
    "    def test(self, testing_data, testing_labels):\n",
    "        total_examples = len(testing_labels);\n",
    "        total_correct = 0;\n",
    "        for example in range(total_examples):\n",
    "            if (testing_labels[example] == self.predict(testing_data[example])):\n",
    "                total_correct = total_correct + 1;\n",
    "        accuracy = total_correct / total_examples;\n",
    "        return (str(total_correct) + \" / \" + str(total_examples) + \" = \" + str(accuracy))\n",
    "    \n",
    "    \"\"\"\n",
    "    print_network prints the weights, biases, and activations for the entire network. \n",
    "        An argument of 1 enables debugging mode, which prints more information, \n",
    "        including errors and gradients.\n",
    "    \"\"\"\n",
    "    def print_network(self, debug=0):\n",
    "        print(\"Number of layers: \");\n",
    "        print(self.num_layers);\n",
    "        print(\"Weights: \")\n",
    "        for layer in self.weights:\n",
    "            print(layer)\n",
    "        print(\"\\nBiases:\" )\n",
    "        for layer in self.biases:\n",
    "            print(layer)\n",
    "        print(\"\\nActivations:\")\n",
    "        for layer in self.activations:\n",
    "            print(layer);\n",
    "        # Print extra info about the network\n",
    "        if (debug == 1):\n",
    "            print(\"\\nWeighted Inputs:\")\n",
    "            for layer in self.Z:\n",
    "                print(layer)\n",
    "            print(\"\\nErrors:\")\n",
    "            for layer in self.errors:\n",
    "                print(layer)\n",
    "            print(\"\\nBias Gradients:\")\n",
    "            for layer in self.gradC_b:\n",
    "                print(layer)\n",
    "            print(\"\\nWeight Gradients\")\n",
    "            for layer in self.gradC_w:\n",
    "                print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 9028 / 10000 = 0.9028\n",
      "Epoch 2: 9222 / 10000 = 0.9222\n",
      "Epoch 3: 9332 / 10000 = 0.9332\n",
      "Epoch 4: 9355 / 10000 = 0.9355\n",
      "Epoch 5: 9416 / 10000 = 0.9416\n",
      "Epoch 6: 9422 / 10000 = 0.9422\n",
      "Epoch 7: 9400 / 10000 = 0.94\n",
      "Epoch 8: 9431 / 10000 = 0.9431\n",
      "Epoch 9: 9424 / 10000 = 0.9424\n",
      "Epoch 10: 9452 / 10000 = 0.9452\n",
      "Done Training\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TEST NETWORK CREATION\n",
    "\"\"\"\n",
    "test = Simple_NN([784, 30, 80, 10]);\n",
    "    \n",
    "batch_size = 10;\n",
    "num_epochs = 10;\n",
    "learning_rate = 3;\n",
    "\n",
    "test.train(training_examples, testing_examples, batch_size, num_epochs, learning_rate);\n",
    "print(\"Done Training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9449 / 10000 = 0.9449\n"
     ]
    }
   ],
   "source": [
    "result = test.test(validation_data, validation_labels);\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = 3;\n",
    "\n",
    "test_data = testing_data[index];\n",
    "label = testing_labels[index];\n",
    "prediction = test.predict(test_data);\n",
    "\n",
    "image = np.reshape(test_data,(28,28))\n",
    "\n",
    "activations = test.activations[-1];\n",
    "\n",
    "print(\"Network prediction: %d\" % prediction)\n",
    "print(\"Output from np.argmax on activation vector: %d\" % np.argmax(activations))\n",
    "\n",
    "pyplot.imshow(image, cmap='Greys_r');\n",
    "pyplot.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
