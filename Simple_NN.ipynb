{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple_NN\n",
    "Class to create a simple neural network\n",
    "    Activation Function: sigmoid\n",
    "    Learning Algorithm: stochastic gradient descent with backpropagation\n",
    "    Cost Function: Mean squared error\n",
    "\"\"\"\n",
    "class Simple_NN(object):\n",
    "    \"\"\" \n",
    "    INITIALIZE THE NETWORK\n",
    "    \"\"\"\n",
    "    def __init__(self, layers, activation_function=\"sigmoid\"):\n",
    "        \"\"\"\n",
    "        self.layers is a list of numbers where the ith number how many neurons are in\n",
    "        the ith layer of the network.\n",
    "        \"\"\"\n",
    "        self.layers = layers;\n",
    "        self.num_layers = len(layers);\n",
    "        \n",
    "        \"\"\"\n",
    "        self.weights[Layer - 1, input_neuron, output_neuron] = \n",
    "                                            List of weight matrices for each layer.                      \n",
    "        self.biases[Layer - 1, neuron] = \n",
    "                                            List of vectors with biases for each neuron  \n",
    "        FOR EXAMPLE:\n",
    "            self.weights[l, j, i] = weight going into the jth neuron of the lth layer\n",
    "                                    from the ith neuron of the (l-1)st layer \n",
    "            self.biases[l, k] = bias on the kth nuron of the lth layer\n",
    "        NOTE: layer 0 is the input layer, so self.weights[0] is the weights going into layer 1\n",
    "        \"\"\"\n",
    "        self.weights = [];\n",
    "        self.biases = [];\n",
    "        self.Z = [];\n",
    "        self.activations = [];\n",
    "        # Create matrices with correct dimensions \n",
    "        for layer_num in range(1, self.num_layers):\n",
    "            self.weights.append(np.random.randn(layers[layer_num], layers[layer_num - 1]));\n",
    "            self.biases.append(np.random.randn(layers[layer_num]));\n",
    "        \"\"\"\n",
    "        self.activation = string specifying what activation function the neurons will use. \n",
    "        The options are:\n",
    "            sigmoid (default)\n",
    "        \"\"\"\n",
    "        self.activation_function = activation_function;\n",
    "        \n",
    "    \"\"\" \n",
    "    ACTIVATION FUNCTION\n",
    "    For this network, we use the sigmoid function to calculate neuron activation\n",
    "    \"\"\"      \n",
    "    def activation(self, z):\n",
    "        if (self.activation_function == \"sigmoid\"):\n",
    "            return 1.0 / (1 + np.exp(-z));\n",
    "    \n",
    "    def activation_derivative(self, z):\n",
    "        if (self.activation_function == \"sigmoid\"):\n",
    "            return (1 - self.activation(z)) * self.activation(z);\n",
    "        \n",
    "    \"\"\"\n",
    "    TRAINING\n",
    "    Train the network using stochastic gradient descent and backpropagation.\n",
    "    Training data should be given in the following format:\n",
    "        [x11, x12, ..., x1i, y1\n",
    "         x21, x22, ..., x2i, y2\n",
    "         ...\n",
    "         xm1, xm1, ..., xmi, ym]\n",
    "    Where each row corrsponds to a training example with i data points\n",
    "    \"\"\"\n",
    "    def train(self, training_data, batch_size, num_epochs, learning_rate):\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"EPOCH: %d\" % epoch);\n",
    "            # Randomize the order of training examples\n",
    "            np.random.shuffle(training_data);\n",
    "            # Separate inputs from outputs\n",
    "            inputs = training_data[:, :-1]\n",
    "            outputs = training_data[:, -1];\n",
    "            # For each epoch, loop through each batch to use as training data\n",
    "            for batch in range(len(training_data))[::batch_size]:\n",
    "                # For each batch, we calculate activations and use the backpropagation algorithm\n",
    "                # to change the weights and biases using gradient descent\n",
    "                self.Z = [];\n",
    "                self.activations = [];\n",
    "                # Create matrix out of all training inputs in the batch\n",
    "                # If the first layer of the network has k neurons, and each training\n",
    "                # example has i data points, then weights will be a kxi matrix \n",
    "                # so Wx_j = kx1 vector.\n",
    "                # To apply W to all input vectors, we can multiply WX where\n",
    "                # X is the ixm matrix containing all m training examples as columns\n",
    "                X = inputs[batch : batch + batch_size];\n",
    "                X = np.transpose(X);\n",
    "                Y = outputs[batch : batch + batch_size];\n",
    "\n",
    "                # *** DEBUGGING  ***\n",
    "                print('Batch #%d' % batch);\n",
    "                print('X');\n",
    "                print(X);\n",
    "                print('Y')\n",
    "                print(Y);\n",
    "                # FEEDFORWARD\n",
    "                \"\"\"\n",
    "                self.Z[layer, training_example, neuron] = \n",
    "                                            List of vectors with weighted inputs to the neurons\n",
    "                self.activations[layer, training_example, neuron] = \n",
    "                                            List of vectors with activations for each neuron\n",
    "                \"\"\"\n",
    "                # Calclate outputs going forwards through the network\n",
    "                for layer in range(self.num_layers - 1):\n",
    "                    if layer == 0:\n",
    "                        # Feed inputs to the network\n",
    "                        prev_activations = X;\n",
    "                    else:\n",
    "                        prev_activations = self.activations[layer - 1];\n",
    "                    # Bias matrix where each column is a copy of the bias vector is needed\n",
    "                    # to add bias terms for each training example. \n",
    "                    one_vector = np.ones(batch_size);\n",
    "                    bias_matrix = np.outer(self.biases[layer], one_vector);\n",
    "                    self.Z.append(np.dot(self.weights[layer], prev_activations) + bias_matrix);\n",
    "                    self.activations.append(self.activation(self.Z[layer]));\n",
    "            \n",
    "                    # Backpropagation\n",
    "                    pass\n",
    "                    # Gradient Descent\n",
    "    \n",
    "    \"\"\" \n",
    "    TODO: \n",
    "        test(testing_data)\n",
    "    \"\"\"\n",
    "    def print_network(self):\n",
    "        print(\"Weights: \")\n",
    "        for layer in self.weights:\n",
    "            print(layer)\n",
    "        print(\"\\nBiases:\" )\n",
    "        for layer in self.biases:\n",
    "            print(layer)\n",
    "        print(\"\\nWeighted Inputs:\")\n",
    "        for layer in self.Z:\n",
    "            print(layer)\n",
    "        print(\"\\nActivations:\")\n",
    "        for layer in self.activations:\n",
    "            print(layer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n",
      "Batch #0\n",
      "X\n",
      "[[ 1 21]\n",
      " [ 2 22]\n",
      " [ 3 23]\n",
      " [ 4 24]]\n",
      "Y\n",
      "[[20]\n",
      " [40]]\n",
      "Batch #2\n",
      "X\n",
      "[[11]\n",
      " [12]\n",
      " [13]\n",
      " [14]]\n",
      "Y\n",
      "[[30]]\n",
      "EPOCH: 1\n",
      "Batch #0\n",
      "X\n",
      "[[11 21]\n",
      " [12 22]\n",
      " [13 23]\n",
      " [14 24]]\n",
      "Y\n",
      "[[30]\n",
      " [40]]\n",
      "Batch #2\n",
      "X\n",
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "Y\n",
      "[[20]]\n",
      "EPOCH: 2\n",
      "Batch #0\n",
      "X\n",
      "[[21  1]\n",
      " [22  2]\n",
      " [23  3]\n",
      " [24  4]]\n",
      "Y\n",
      "[[40]\n",
      " [20]]\n",
      "Batch #2\n",
      "X\n",
      "[[11]\n",
      " [12]\n",
      " [13]\n",
      " [14]]\n",
      "Y\n",
      "[[30]]\n",
      "After training\n",
      "Weights: \n",
      "[[ 1.17078602 -0.17434719  0.89476906 -0.28776759]\n",
      " [-0.83589868 -0.45646129 -1.35406509  1.00579799]\n",
      " [-0.64313304  0.6982566  -0.86917868  0.41219644]\n",
      " [-1.38281037 -0.25042522  1.26309675 -0.01167252]\n",
      " [-0.87556566 -0.20768007  0.40072757  1.00282825]\n",
      " [ 0.66959062 -0.7590287   0.53315077  0.16071834]]\n",
      "[[-0.56120841 -0.54132984 -1.28661186  0.19411639  0.41357819 -0.44713059]\n",
      " [-1.63467226 -0.55831178  0.18517031 -0.02010543 -0.96289863  0.17955346]]\n",
      "\n",
      "Biases:\n",
      "[ 0.96292082  0.10285858  0.15880407  0.31242535 -0.37943537  2.46271064]\n",
      "[ 0.40598243 -0.60168612]\n",
      "\n",
      "Weighted Inputs:\n",
      "[[ 19.35265227  19.35265227]\n",
      " [-18.09123659 -18.09123659]\n",
      " [ -4.06515279  -4.06515279]\n",
      " [ -1.64674883  -1.64674883]\n",
      " [  6.74623555   6.74623555]\n",
      " [  9.90087986   9.90087986]]\n",
      "[[-0.17958846 -0.17958846]\n",
      " [-3.01870622 -3.01870622]]\n",
      "\n",
      "Activations:\n",
      "[[  9.99999996e-01   9.99999996e-01]\n",
      " [  1.39019515e-08   1.39019515e-08]\n",
      " [  1.68708570e-02   1.68708570e-02]\n",
      " [  1.61548839e-01   1.61548839e-01]\n",
      " [  9.98826084e-01   9.98826084e-01]\n",
      " [  9.99949872e-01   9.99949872e-01]]\n",
      "[[ 0.45522317  0.45522317]\n",
      " [ 0.04658791  0.04658791]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TEST NETWORK CREATION\n",
    "\"\"\"\n",
    "test = Simple_NN([4, 6, 2]);\n",
    "random_data = np.matrix('1, 2, 3, 4, 20; 11, 12, 13, 14, 30; 21, 22, 23, 24, 40')\n",
    "batch_size = 2;\n",
    "num_epochs = 3;\n",
    "learning_rate = 0.5;\n",
    "test.train(random_data, batch_size, num_epochs, learning_rate);\n",
    "\n",
    "print(\"After training\");\n",
    "test.print_network();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
