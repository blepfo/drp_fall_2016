{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Simple_NN\n",
    "Class to create a simple neural network\n",
    "    Learning Algorithm: stochastic gradient descent with backpropagation\n",
    "    Activation Function: sigmoid\n",
    "    Cost Function: MSE\n",
    "\"\"\"\n",
    "class Simple_NN(object):\n",
    "    \"\"\" \n",
    "    INITIALIZE THE NETWORK\n",
    "    \"\"\"\n",
    "    def __init__(self, layers, activation_function=\"sigmoid\", cost_function=\"MSE\"):\n",
    "        \"\"\"\n",
    "        self.layers is a list of numbers where the ith number how many neurons are in\n",
    "        the ith layer of the network.\n",
    "        \"\"\"\n",
    "        self.layers = layers;\n",
    "        self.num_layers = len(layers);\n",
    "        \n",
    "        \"\"\"\n",
    "        self.weights[Layer - 1, input_neuron, output_neuron] = \n",
    "                                            List of weight matrices for each layer.                      \n",
    "        self.biases[Layer - 1, neuron] = \n",
    "                                            List of vectors with biases for each neuron  \n",
    "        FOR EXAMPLE:\n",
    "            self.weights[layer, j, i] = weight going into the jth neuron of the lth layer\n",
    "                                    from the ith neuron of the (l-1)st layer \n",
    "            self.biases[layer, k] = bias on the kth nuron of the lth layer\n",
    "        NOTE: layer 0 is the input layer, so self.weights[0] is the weights going into layer 1\n",
    "        \"\"\"\n",
    "        self.weights = [];\n",
    "        self.biases = [];\n",
    "        self.Z = [];\n",
    "        self.activations = [];\n",
    "        # Create matrices with correct dimensions \n",
    "        for layer_num in range(1, self.num_layers):\n",
    "            self.weights.append(np.random.randn(layers[layer_num], layers[layer_num - 1]));\n",
    "            self.biases.append(np.random.randn(layers[layer_num]));\n",
    "        \"\"\"\n",
    "        self.activation = string specifying what activation function the neurons will use. \n",
    "            The options are:\n",
    "            sigmoid (default)\n",
    "        self.cost_function = string specifying hat cost function will be used for the \n",
    "            network.\n",
    "            The options are:\n",
    "            MSE (default)\n",
    "        \"\"\"\n",
    "        self.activation_function = activation_function;\n",
    "        self.cost_function = cost_function;\n",
    "        self.errors = [];\n",
    "        self.gradC_b = [];\n",
    "        self.gradC_w = [];\n",
    " \n",
    "    \"\"\"\n",
    "    TRAINING\n",
    "    Train the network using stochastic gradient descent and backpropagation.\n",
    "    Training data should be given in the following format:\n",
    "        [x11, x12, ..., x1i, y1\n",
    "         x21, x22, ..., x2i, y2\n",
    "         ...\n",
    "         xm1, xm1, ..., xmi, ym]\n",
    "    Where each row corrsponds to a training example with i data points\n",
    "    \"\"\"\n",
    "    def train(self, training_data, batch_size, num_epochs, learning_rate):\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"EPOCH: %d\" % epoch);\n",
    "            # Randomize the order of training examples\n",
    "            np.random.shuffle(training_data);\n",
    "            # Separate inputs from outputs\n",
    "            inputs = training_data[:, :-1]\n",
    "            outputs = training_data[:, -1];\n",
    "            # For each epoch, loop through each batch to use as training data\n",
    "            for batch in range(len(training_data))[::batch_size]:\n",
    "                \"\"\"\n",
    "                For each batch, we calculate activations and use the backpropagation\n",
    "                algorithm to change the weights and biases using gradient descent\n",
    "                Create matrix out of all training inputs in the batch\n",
    "                To apply W to all input vectors, we can multiply WX where\n",
    "                X is the ixm matrix containing all m training examples as columns\n",
    "                \"\"\"\n",
    "                X = inputs[batch : batch + batch_size];\n",
    "                X = np.transpose(X);\n",
    "                Y = outputs[batch : batch + batch_size];\n",
    "                # FEEDFORWARD\n",
    "                self.evaluate(X);\n",
    "                # BACKPROPAGATION\n",
    "                self.backpropagate(X, Y);\n",
    "                # GRADIENT DESCENT\n",
    "                self.grad_descent();\n",
    "    \n",
    "    \"\"\"\n",
    "    EVALUATE\n",
    "    Takes an input vector X and computes the output of the network\n",
    "    \"\"\"\n",
    "    def evaluate(self, X):\n",
    "        \"\"\"\n",
    "        self.Z[layer, neuron, training_example] = \n",
    "                        List of vectors with weighted inputs to the neurons\n",
    "        self.activations[layer, neuron, training_Example] = \n",
    "                        List of vectors with activations for each neuron\n",
    "        \"\"\"\n",
    "        self.Z = [];\n",
    "        self.activations = [];\n",
    "        # Calclate outputs going forwards through the network\n",
    "        for layer in range(self.num_layers - 1):\n",
    "            if layer == 0:\n",
    "                # Feed inputs to the network\n",
    "                prev_activations = X;\n",
    "            else:\n",
    "                prev_activations = self.activations[layer - 1];\n",
    "            # Bias matrix where each column is a copy of the bias vector is needed\n",
    "            # to add bias terms for each training example. \n",
    "            one_vector = np.ones(batch_size);\n",
    "            bias_matrix = np.outer(self.biases[layer], one_vector);  \n",
    "            self.Z.append(np.dot(self.weights[layer], prev_activations) + bias_matrix);\n",
    "            self.activations.append(self.activation(self.Z[layer]));\n",
    "        \n",
    "    \"\"\"\n",
    "    BACKPROPAGATE\n",
    "    Using input vector X and output vector Y, calculates the following instance variables:\n",
    "        self.errors[layer, neuron, training_example] - error in each neuron\n",
    "        self.gradC_b[layer] - average gradient wrt biases over all training examples\n",
    "        self.gradC_w[layer, input_neuron, output_neuron] - average gradient wrt weights\n",
    "    \"\"\"\n",
    "    def backpropagate(self, X, Y):\n",
    "        # Calculate output error matrix so the [i, j]th entry contains the\n",
    "        # error for the ith neuron in the output layer for the jth training\n",
    "        # example\n",
    "        output_error = np.multiply(\n",
    "                             self.cost_derivative(self.activations[-1], Y),\n",
    "                             self.activation_derivative(self.Z[-1]));\n",
    "        # Backpropogate: we create the errors matrix which is indexed\n",
    "        # in the form errors[layer, neuron, training_example]\n",
    "        self.errors = [output_error];\n",
    "        # Note that in the loop, we use negative subscripts to go through the \n",
    "        # layers from output towards input. We therefore start at layer [-2], the\n",
    "        # second to last layer\n",
    "        for layer in range(2, self.num_layers):               \n",
    "            # For each layer, calculate errors in previous layer\n",
    "            previous_errors = np.multiply(\n",
    "                                np.dot(\n",
    "                                    np.transpose(self.weights[-layer + 1]),\n",
    "                                    self.errors[0]), \n",
    "                                self.activation_derivative(self.Z[-layer])); \n",
    "            # Add previous errors to the beginning of the error matrix list\n",
    "            self.errors.insert(0, previous_errors);\n",
    "        # Calculate gradients of cost function\n",
    "        self.gradC_b = [];\n",
    "        self.gradC_w = [];\n",
    "        for layer in range(self.num_layers - 1):\n",
    "            \"\"\"\n",
    "            gradC_b[layer, neuron] = \n",
    "            Gradient of cost wrt biases for a layer is just the\n",
    "            vector of errors for that layer.\n",
    "            When we compute the average over all training examples using \n",
    "            np.average(matrix, 1), we get a column vector. In order to\n",
    "            correctly subtract this from the row-vector of biases, \n",
    "            we transpose the gradient vectors.\n",
    "            \"\"\"\n",
    "            self.gradC_b.append(np.average(self.errors[layer],1));\n",
    "            \"\"\"\n",
    "            sum_of_weights[layer, j, k] will contain the partial derivative \n",
    "            of cost wrt the weight from the kth neuron in layer - 1 to the jth\n",
    "            neuron in layer summed over all training examples. That is,\n",
    "            sum_of_weightes[layer, j, k] = [sum over training examples dC/dw_j,k]\n",
    "            \"\"\"\n",
    "            if (layer == 0):\n",
    "               prev_activations = X;\n",
    "            else:\n",
    "               prev_activations = self.activations[layer - 1];\n",
    "            sum_of_weights = np.dot(\n",
    "                                    self.errors[layer],\n",
    "                                    np.transpose(prev_activations));\n",
    "            \"\"\"\n",
    "            gradC_w [layer, input_neuron, output_neuron] is also \n",
    "                averaged over all traininge examples in the batch\n",
    "            \"\"\"\n",
    "            self.gradC_w.append((1 / batch_size) * sum_of_weights);\n",
    "    \n",
    "    \"\"\"\n",
    "    GRADIENT DESCENT\n",
    "    Use the gradients computed in backpropagate() to update the weights and\n",
    "    biases of the network.\n",
    "    \"\"\"\n",
    "    def grad_descent(self):\n",
    "        for layer in range(self.num_layers - 1):\n",
    "            self.biases[layer] = np.subtract(self.biases[layer],\n",
    "                        np.transpose(np.multiply(learning_rate, self.gradC_b[layer])));\n",
    "            self.weights[layer] = np.subtract(self.weights[layer],\n",
    "                        np.multiply(learning_rate, self.gradC_w[layer]));\n",
    "        \n",
    "    \"\"\" \n",
    "    ACTIVATION FUNCTION\n",
    "    For this network, we use the sigmoid function to calculate neuron activation\n",
    "    In general, we assume the input z will be a matrix where the [i,j]th entry is the\n",
    "    ith neuron in the jth training example\n",
    "    \"\"\"      \n",
    "    def activation(self, z):\n",
    "        if (self.activation_function == \"sigmoid\"):\n",
    "            return 1.0 / (1 + np.exp(-z));\n",
    "    \n",
    "    def activation_derivative(self, z):\n",
    "        if (self.activation_function == \"sigmoid\"):\n",
    "            return np.multiply((1 - self.activation(z)), self.activation(z));\n",
    "    \"\"\"\n",
    "    COST FUNCTION\n",
    "    We assume that activations is a 2D matrix where each column corresponds to the activations\n",
    "    for a specific training example and each row corresponds to a specific neuron\n",
    "\n",
    "    We assume for now that outputs for the network are disjoint categories, so only\n",
    "    one output neuron should fire at a time. the output_matrix function turns the output\n",
    "    vector where each entry corresponds to a training example into \n",
    "    \n",
    "    Both calculate_cost and cost_derivative return a row vector where the ith entry\n",
    "    is the cost/cost derivative for training example i\n",
    "    \"\"\"\n",
    "    def output_matrix(self, activations, outputs):\n",
    "        # output_matrix[neuron, training_example]\n",
    "        output_matrix = np.zeros((activations.shape[0], outputs.shape[0]));\n",
    "        for training_example, output in enumerate(outputs):\n",
    "            output_matrix[output, training_example] = 1;\n",
    "        return output_matrix;\n",
    "    \n",
    "    def calculate_cost(self, activations, outputs):\n",
    "        outputs_matrix = self.output_matrix(activations, outputs);\n",
    "        if (self.cost_function == \"MSE\"):\n",
    "            squared_errors = np.square(activations - output_matrix);\n",
    "            # Average sum of squared errors over each training example\n",
    "            return 0.5 * np.average(np.sum(squared_errors, 0));\n",
    "    \"\"\"\n",
    "    Derivative of cost functions with respect to activations\n",
    "    Each entry [i, j] in the resulting matrix will be the gradient of the cost function\n",
    "    with respect to the activation of the ith neuron in the jth training example\n",
    "    \"\"\"\n",
    "    def cost_derivative(self, activations, outputs):\n",
    "        output_matrix = self.output_matrix(activations, outputs);\n",
    "        if (self.cost_function == \"MSE\"):\n",
    "            return (activations - output_matrix);\n",
    "       \n",
    "    \"\"\" \n",
    "    TODO: \n",
    "        predict(data)\n",
    "        test(testing_data)\n",
    "    \"\"\"\n",
    "    \n",
    "    def print_network(self):\n",
    "        print(\"Number of layers: \");\n",
    "        print(self.num_layers);\n",
    "        print(\"Weights: \")\n",
    "        for layer in self.weights:\n",
    "            print(layer)\n",
    "        print(\"\\nBiases:\" )\n",
    "        for layer in self.biases:\n",
    "            print(layer)\n",
    "        print(\"\\nWeighted Inputs:\")\n",
    "        for layer in self.Z:\n",
    "            print(layer)\n",
    "        print(\"\\nActivations:\")\n",
    "        for layer in self.activations:\n",
    "            print(layer);\n",
    "        print(\"\\nErrors:\")\n",
    "        for layer in self.errors:\n",
    "            print(layer)\n",
    "        print(\"\\nBias Gradients:\")\n",
    "        for layer in self.gradC_b:\n",
    "            print(layer)\n",
    "        print(\"\\nWeight Gradients\")\n",
    "        for layer in self.gradC_w:\n",
    "            print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "BEGIN TRAINING\n",
      "\n",
      "EPOCH: 0\n",
      "EPOCH: 1\n",
      "EPOCH: 2\n",
      "After training\n",
      "Number of layers: \n",
      "5\n",
      "Weights: \n",
      "[[ 0.3758496  -0.34491384 -0.38985529]\n",
      " [-0.89501029 -0.67887755  1.76574667]\n",
      " [ 0.09788202  0.31368507 -0.94465044]\n",
      " [-0.27211998  1.49180206 -1.88507685]\n",
      " [-2.15503294 -0.33280391  1.84987749]\n",
      " [ 0.75713262 -0.02851398  0.76095766]]\n",
      "[[ 0.94363366 -1.06696093 -1.025694   -0.60055595 -0.05856035  0.26025595]\n",
      " [ 1.51779832 -0.0303841   0.03658494 -0.02603157  0.18149658  2.4174996 ]\n",
      " [-0.1675492  -0.34339186 -2.07730675  1.11308277 -1.29875818 -1.46739986]\n",
      " [ 1.04159957 -1.16847651 -0.22745171 -1.65518118 -0.22472392  0.03541468]\n",
      " [ 0.43227309  1.47173115  1.19551283  0.65828623  1.49929719 -1.11640192]]\n",
      "[[-0.89781886 -0.35802998  0.94906852  1.26537511 -0.31954126]\n",
      " [ 0.65323819  0.98217063 -1.00264479  0.70754262  0.72983851]\n",
      " [-0.9388299   1.12934797 -0.00365666  0.67244556  0.29024175]\n",
      " [ 0.46301718 -0.51307976  0.42640043  0.48235926  0.97467606]\n",
      " [-0.09841142 -0.99478782 -0.46121232 -0.21602178 -0.22531904]\n",
      " [ 0.99960003  0.49958649 -1.32690762  0.10738629  1.12935946]\n",
      " [-0.09471999 -0.70804559 -0.21160205  0.51177848  0.84626807]\n",
      " [-0.21882537 -0.350792    0.57903288  0.32699414  0.74043257]\n",
      " [ 0.80563664 -0.04037739 -0.28321069 -0.68134996 -0.8591049 ]\n",
      " [ 0.0307183  -1.53447889 -0.7344986  -0.31163919  1.19830329]]\n",
      "[[-1.60391306 -0.46071372  1.31501006 -1.29795202  0.23088017 -0.83415107\n",
      "  -1.51653606  1.70425061 -0.33770667 -0.25787097]\n",
      " [-0.74383723 -0.42490769  0.52853901  0.18362673  1.26506223 -0.11526747\n",
      "  -0.93974287  1.01827309  1.92505496 -0.92457765]\n",
      " [ 1.38101889 -0.45418438 -1.79177284 -0.86947771  0.83987782 -0.83609933\n",
      "  -1.28187335 -0.51662786 -1.73787341  2.11980562]\n",
      " [ 0.1847431  -0.98446162 -0.29827134 -0.41615166 -1.64139127  1.1222675\n",
      "  -0.00497077 -0.22527496  0.66173178 -0.32455063]]\n",
      "\n",
      "Biases:\n",
      "[[ 0.73100226 -2.11871762  0.06484054 -1.69148204  0.29128921 -0.14724135]]\n",
      "[[-0.20744253 -0.01207336  0.43292684  2.25229875  0.67215637]]\n",
      "[[-0.87287869 -0.48505124  0.92664451 -0.66938059 -1.02895703 -0.06645167\n",
      "   1.09730943 -1.97660322  0.92614639  0.39447139]]\n",
      "[[-1.00937672 -0.61747402  0.97516099  0.55614111]]\n",
      "\n",
      "Weighted Inputs:\n",
      "[[-11.49505895  -4.33229552]\n",
      " [  6.91399976   2.93190491]\n",
      " [-18.03160533  -7.37290174]\n",
      " [-24.59709155 -11.28918368]\n",
      " [-15.91470414  -3.28255844]\n",
      " [ 47.52302498  17.73149916]]\n",
      "[[-1.01806882 -0.95560837]\n",
      " [ 2.37391065  2.40169039]\n",
      " [-1.37792825 -1.41133146]\n",
      " [ 1.12503407  1.18813192]\n",
      " [ 1.03246417  1.01995671]]\n",
      "[[-0.51535285 -0.51690062]\n",
      " [ 1.45730944  1.4791666 ]\n",
      " [ 2.42819121  2.42603418]\n",
      " [ 0.14595973  0.1514607 ]\n",
      " [-2.39554657 -2.39840469]\n",
      " [ 1.30708527  1.32606857]\n",
      " [ 1.399643    1.4019334 ]\n",
      " [-1.45614651 -1.46076082]\n",
      " [-0.12905756 -0.12352495]\n",
      " [-0.48666734 -0.49210374]]\n",
      "[[-3.25523496 -3.26274884]\n",
      " [-0.74338145 -0.74234082]\n",
      " [-2.70109872 -2.7120392 ]\n",
      " [ 0.25051519  0.25209427]]\n",
      "\n",
      "Activations:\n",
      "[[  1.01801671e-05   1.29670037e-02]\n",
      " [  9.99007212e-01   9.49401262e-01]\n",
      " [  1.47561580e-08   6.27648927e-04]\n",
      " [  2.07787237e-11   1.25073237e-05]\n",
      " [  1.22555204e-07   3.61744082e-02]\n",
      " [  1.00000000e+00   9.99999980e-01]]\n",
      "[[ 0.26540374  0.27775833]\n",
      " [ 0.9148161   0.91695611]\n",
      " [ 0.20134194  0.19602414]\n",
      " [ 0.75492129  0.76640679]\n",
      " [ 0.73739335  0.73496417]]\n",
      "[[ 0.37393953  0.37357726]\n",
      " [ 0.81112082  0.81444667]\n",
      " [ 0.91895192  0.91879112]\n",
      " [ 0.53642529  0.53779295]\n",
      " [ 0.08351292  0.08329443]\n",
      " [ 0.78702501  0.79018959]\n",
      " [ 0.80212723  0.80249051]\n",
      " [ 0.18905741  0.18835099]\n",
      " [ 0.46778032  0.46915797]\n",
      " [ 0.38067897  0.3793981 ]]\n",
      "[[ 0.03713923  0.03687147]\n",
      " [ 0.32226516  0.32249248]\n",
      " [ 0.06290855  0.06226668]\n",
      " [ 0.5623033   0.5626919 ]]\n",
      "\n",
      "Errors:\n",
      "[[ -5.20860986e-08   8.69557019e-05]\n",
      " [ -3.86466912e-06   8.15407202e-04]\n",
      " [ -1.33780082e-10   1.64636826e-05]\n",
      " [  9.04819176e-14  -6.96653679e-08]\n",
      " [ -9.33712871e-10   7.05789269e-04]\n",
      " [  0.00000000e+00  -3.10937351e-10]]\n",
      "[[ 0.00151631 -0.00831165]\n",
      " [-0.00044492 -0.00114455]\n",
      " [ 0.00166351 -0.00223738]\n",
      " [-0.0036772   0.01001554]\n",
      " [-0.00407796  0.01287659]]\n",
      "[[-0.0164066   0.03278157]\n",
      " [ 0.01103881 -0.01094613]\n",
      " [ 0.00464949 -0.00900646]\n",
      " [ 0.01283776 -0.02175838]\n",
      " [ 0.02059667 -0.03137126]\n",
      " [-0.02262136  0.02842617]\n",
      " [-0.01176637  0.0213884 ]\n",
      " [ 0.0146996  -0.02764505]\n",
      " [ 0.0140133  -0.04937178]\n",
      " [-0.00552165  0.02373031]]\n",
      "[[ 0.0013281   0.00130938]\n",
      " [ 0.07038604 -0.14802935]\n",
      " [ 0.00370853  0.00363572]\n",
      " [-0.10772517  0.13846144]]\n",
      "\n",
      "Bias Gradients:\n",
      "[[  4.34518079e-05]\n",
      " [  4.05771266e-04]\n",
      " [  8.23177440e-06]\n",
      " [ -3.48326387e-08]\n",
      " [  3.52894168e-04]\n",
      " [ -1.55468675e-10]]\n",
      "[[-0.00339767]\n",
      " [-0.00079473]\n",
      " [-0.00028693]\n",
      " [ 0.00316917]\n",
      " [ 0.00439932]]\n",
      "[[  8.18748446e-03]\n",
      " [  4.63387594e-05]\n",
      " [ -2.17848747e-03]\n",
      " [ -4.46031098e-03]\n",
      " [ -5.38729509e-03]\n",
      " [  2.90240559e-03]\n",
      " [  4.81101478e-03]\n",
      " [ -6.47272463e-03]\n",
      " [ -1.76792421e-02]\n",
      " [  9.10432888e-03]]\n",
      "[[ 0.00131874]\n",
      " [-0.03882166]\n",
      " [ 0.00367212]\n",
      " [ 0.01536814]]\n",
      "\n",
      "Weight Gradients\n",
      "[[  4.77449026e-04   5.20900834e-04   5.64352642e-04]\n",
      " [  4.42483724e-03   4.83060850e-03   5.23637977e-03]\n",
      " [  9.05481806e-05   9.87799550e-05   1.07011729e-04]\n",
      " [ -3.83158121e-07  -4.17990760e-07  -4.52823398e-07]\n",
      " [  3.88182651e-03   4.23472068e-03   4.58761484e-03]\n",
      " [ -1.71015543e-09  -1.86562410e-09  -2.02109278e-09]]\n",
      "[[ -5.38808736e-05  -3.18814316e-03  -2.60838760e-06  -5.19782265e-08\n",
      "   -1.50334399e-04  -3.39766986e-03]\n",
      " [ -7.42296090e-06  -7.65557722e-04  -3.59191274e-07  -7.15763732e-09\n",
      "   -2.07017484e-05  -7.94734975e-04]\n",
      " [ -1.44975854e-05  -2.31156013e-04  -7.02132081e-07  -1.39917962e-08\n",
      "   -4.04678339e-05  -2.86934518e-04]\n",
      " [  6.49170716e-05   2.91760943e-03   3.14309514e-06   6.26337783e-08\n",
      "    1.81152937e-04   3.16917089e-03]\n",
      " [  8.34646639e-05   4.07557190e-03   4.04096012e-06   8.05258223e-08\n",
      "    2.32901334e-04   4.39931719e-03]]\n",
      "[[  2.37549048e-03   7.52511908e-03   1.56132103e-03   6.36916275e-03\n",
      "    5.99758039e-03]\n",
      " [ -5.53189979e-05   3.06790286e-05   3.84346085e-05  -2.78787812e-05\n",
      "    4.74649556e-05]\n",
      " [ -6.33814321e-04  -2.00255238e-03  -4.14673591e-04  -1.69630846e-03\n",
      "   -1.59546300e-03]\n",
      " [ -1.31819118e-03  -4.10364589e-03  -8.40194273e-04  -3.49213670e-03\n",
      "   -3.26257612e-03]\n",
      " [ -1.62359783e-03  -4.96195171e-03  -1.00127536e-03  -4.24714111e-03\n",
      "   -3.93445233e-03]\n",
      " [  9.45906209e-04   2.68558353e-03   5.08793576e-04   2.35433224e-03\n",
      "    2.10568841e-03]\n",
      " [  1.40898381e-03   4.42407949e-03   9.11789395e-04   3.75476576e-03\n",
      "    3.52163214e-03]\n",
      " [ -1.88865699e-03  -5.95093307e-03  -1.22972547e-03  -5.04515628e-03\n",
      "   -4.73936667e-03]\n",
      " [ -4.99712114e-03  -1.62260834e-02  -3.42829817e-03  -1.36299662e-02\n",
      "   -1.29765890e-02]\n",
      " [  2.56291209e-03   8.35417822e-03   1.76998667e-03   7.00932898e-03\n",
      "    6.68464894e-03]]\n",
      "[[ 0.00049289  0.00107183  0.00121175  0.0007083   0.00010999  0.00103995\n",
      "   0.00105803  0.00024885  0.00061778  0.00050118]\n",
      " [-0.01449014 -0.03173522 -0.03566333 -0.02092615 -0.00322594 -0.03078784\n",
      "  -0.0311668  -0.00728724 -0.01826197 -0.01468379]\n",
      " [ 0.00137249  0.00298458  0.00337421  0.00197231  0.00030627  0.00289581\n",
      "   0.00294617  0.00069296  0.00172025  0.00139557]\n",
      " [ 0.00572167  0.01269567  0.01411145  0.00833854  0.00126831  0.0123142\n",
      "   0.01235235  0.00285655  0.00728429  0.00576165]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TEST NETWORK CREATION\n",
    "\"\"\"\n",
    "test = Simple_NN([3, 6, 5, 10, 4]);\n",
    "random_data = np.matrix('1, 2, 3, 0; 11, 12, 13, 1; 21, 22, 23, 2; 31, 32, 33, 3')\n",
    "batch_size = 2;\n",
    "num_epochs = 3;\n",
    "learning_rate = 0.5;\n",
    "\n",
    "print(\"\\n\\nBEGIN TRAINING\\n\")\n",
    "\n",
    "test.train(random_data, batch_size, num_epochs, learning_rate);\n",
    "\n",
    "print(\"After training\");\n",
    "test.print_network();\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
